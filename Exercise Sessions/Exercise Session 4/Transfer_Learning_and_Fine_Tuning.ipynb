{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoeDebrois/Artificial-NN-and-Deep-Learning/blob/main/Transfer_Learning_and_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRyyx0cfzGF9"
      },
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Lecture 4b: Transfer Learning and Fine Tuning\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1gsJ4h701PWou3B_JsjbfhIZnefsn2mGN\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSLbdLvhDRx"
      },
      "source": [
        "## üåê Connect Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoaLQpvChLpb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2024-2025] AN2DL/Lecture 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdD_8Vyswkwf"
      },
      "source": [
        "## ‚öôÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_S1JfaW8bIN"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53DAEfQuI41_"
      },
      "source": [
        "## ‚è≥ Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3urwCS_UZmbA"
      },
      "outputs": [],
      "source": [
        "def load_oxford_pets():\n",
        "    # Load dataset with TensorFlow Datasets, obtaining dataset info\n",
        "    dataset, info = tfds.load(\n",
        "        'oxford_iiit_pet',\n",
        "        with_info=True,\n",
        "        as_supervised=True\n",
        "    )\n",
        "\n",
        "    # Split dataset into training and test sets\n",
        "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "    # Define image preprocessing function\n",
        "    def preprocess(image, label):\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        height = tf.shape(image)[0]\n",
        "        width = tf.shape(image)[1]\n",
        "        crop_size = tf.minimum(height, width)\n",
        "        height_offset = (height - crop_size) // 2\n",
        "        width_offset = (width - crop_size) // 2\n",
        "\n",
        "        # Centre-crop and resize image, normalising pixel values\n",
        "        image = tf.image.crop_to_bounding_box(\n",
        "            image,\n",
        "            height_offset,\n",
        "            width_offset,\n",
        "            crop_size,\n",
        "            crop_size\n",
        "        )\n",
        "        image = tf.image.resize(image, [128, 128])\n",
        "        image = image / 255.0\n",
        "        return image, label\n",
        "\n",
        "    # Initialise NumPy arrays for training and test sets\n",
        "    n_train = info.splits['train'].num_examples\n",
        "    n_test = info.splits['test'].num_examples\n",
        "    img_shape = (128, 128, 3)\n",
        "\n",
        "    X_train = np.zeros((n_train, *img_shape), dtype=np.float32)\n",
        "    y_train = np.zeros(n_train, dtype=np.int64)\n",
        "    X_test = np.zeros((n_test, *img_shape), dtype=np.float32)\n",
        "    y_test = np.zeros(n_test, dtype=np.int64)\n",
        "\n",
        "    # Batch-process and store training data\n",
        "    train_dataset = train_dataset.map(preprocess).batch(32)\n",
        "    idx = 0\n",
        "    for batch_images, batch_labels in train_dataset:\n",
        "        batch_size = batch_images.shape[0]\n",
        "        X_train[idx:idx + batch_size] = batch_images.numpy()\n",
        "        y_train[idx:idx + batch_size] = batch_labels.numpy()\n",
        "        idx += batch_size\n",
        "\n",
        "        # Optional: print progress for training data\n",
        "        if idx % 500 == 0:\n",
        "            print(f\"Processed {idx}/{n_train} training images\")\n",
        "\n",
        "    # Batch-process and store test data\n",
        "    test_dataset = test_dataset.map(preprocess).batch(32)\n",
        "    idx = 0\n",
        "    for batch_images, batch_labels in test_dataset:\n",
        "        batch_size = batch_images.shape[0]\n",
        "        X_test[idx:idx + batch_size] = batch_images.numpy()\n",
        "        y_test[idx:idx + batch_size] = batch_labels.numpy()\n",
        "        idx += batch_size\n",
        "\n",
        "        # Optional: print progress for test data\n",
        "        if idx % 500 == 0:\n",
        "            print(f\"Processed {idx}/{n_test} test images\")\n",
        "\n",
        "    # Retrieve and format class names\n",
        "    class_names = list(map(str.lower, info.features['label'].names))\n",
        "    return (X_train, y_train), (X_test, y_test), class_names\n",
        "\n",
        "# Execute function and load data\n",
        "(X_train, y_train), (X_test, y_test), class_names = load_oxford_pets()\n",
        "\n",
        "# Display data shapes for training and test sets\n",
        "print(\"Training set shape (images):\", X_train.shape)\n",
        "print(\"Training set shape (labels):\", y_train.shape)\n",
        "print(\"Test set shape (images):\", X_test.shape)\n",
        "print(\"Test set shape (labels):\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîé Inspect Data"
      ],
      "metadata": {
        "id": "Y_jp5hxAZMkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htLTPZ-xaO2J"
      },
      "outputs": [],
      "source": [
        "# Number of images to display\n",
        "num_img = 10\n",
        "\n",
        "# Select random indices from the training set\n",
        "random_indices = random.sample(range(len(X_train)), num_img)\n",
        "\n",
        "# Create subplot layout for images\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "\n",
        "for i, idx in enumerate(random_indices):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "\n",
        "    # Display the image at the selected index\n",
        "    ax.imshow(X_train[idx])\n",
        "\n",
        "    # Add class name as title, formatting it to replace underscores with spaces and capitalise\n",
        "    class_name = class_names[y_train[idx]]\n",
        "    class_name = class_name.replace('_', ' ').title()\n",
        "    ax.set_title(class_name, pad=5)\n",
        "\n",
        "    # Remove axis lines for clearer display\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Te_tNKg3-7"
      },
      "outputs": [],
      "source": [
        "def plot_class_distribution(y_train, y_test, class_names):\n",
        "    # Set seaborn style for the plot\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Calculate class distributions for training and test sets\n",
        "    train_dist = np.bincount(y_train)\n",
        "    test_dist = np.bincount(y_test)\n",
        "\n",
        "    # Create x positions and set bar width\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.35\n",
        "\n",
        "    # Plot bars for training and test distributions\n",
        "    plt.bar(x - width / 2, train_dist, width, label='Training', color='#2ecc71', alpha=0.7)\n",
        "    plt.bar(x + width / 2, test_dist, width, label='Test', color='#3498db', alpha=0.7)\n",
        "\n",
        "    # Customise plot title and labels\n",
        "    plt.title('Class Distribution', pad=20, fontsize=14)\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Images')\n",
        "\n",
        "    # Set class names as x-axis labels with rotation\n",
        "    plt.xticks(x, class_names, rotation=45, ha='right')\n",
        "\n",
        "    # Add legend for training and test distributions\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Adjust layout for optimal spacing\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute function to plot class distribution\n",
        "plot_class_distribution(y_train, y_test, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ScijGfg38b"
      },
      "outputs": [],
      "source": [
        "# Split test set into validation and test sets with stratification\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=seed, test_size=0.5, stratify=y_test)\n",
        "\n",
        "# Convert class labels to categorical format for training, validation, and test sets\n",
        "y_train = tfk.utils.to_categorical(y_train, num_classes=len(class_names))\n",
        "y_val = tfk.utils.to_categorical(y_val, num_classes=len(class_names))\n",
        "y_test = tfk.utils.to_categorical(y_test, num_classes=len(class_names))\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCbZG6izjZi-"
      },
      "source": [
        "## üõ†Ô∏è Train MobileNetV3Small from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0N66p4OI2G8"
      },
      "outputs": [],
      "source": [
        "# Initialise MobileNetV3Small model without pretrained weights, for custom training\n",
        "mobilenet = tfk.applications.MobileNetV3Small(\n",
        "    input_shape=(128, 128, 3),\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    pooling='avg',\n",
        "    include_preprocessing=True\n",
        ")\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "mobilenet.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(mobilenet, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pJtND4OjdOZ"
      },
      "outputs": [],
      "source": [
        "# Create an input layer with shape (128, 128, 3)\n",
        "inputs = tfk.Input(shape=(128, 128, 3), name='input_layer')\n",
        "\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal\"),\n",
        "    tfkl.RandomTranslation(0.2,0.2)\n",
        "], name='preprocessing')\n",
        "\n",
        "x = augmentation(inputs)\n",
        "\n",
        "# Connect MobileNetV3Small to the input\n",
        "x = mobilenet(x)\n",
        "\n",
        "x = tfkl.Dropout(0.3, name='dropout')(x)\n",
        "\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(y_train.shape[-1], activation='softmax', name='dense')(x)\n",
        "\n",
        "# Create a Model connecting input and output\n",
        "model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loZ7T818jdMB"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = X_train*255,\n",
        "    y = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 200,\n",
        "    validation_data = (X_val*255, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)]\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'Pets_EfficientNetV2B0_'+str(final_val_accuracy)+'.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up resources\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2bVC89slWXK"
      },
      "outputs": [],
      "source": [
        "# Create figure and subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))\n",
        "\n",
        "# Plot loss in first subplot\n",
        "ax1.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "ax1.set_title('Categorical Crossentropy')\n",
        "ax1.legend(loc='upper left')\n",
        "ax1.grid(alpha=.3)\n",
        "\n",
        "# Plot accuracy in second subplot\n",
        "ax2.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(history['val_accuracy'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.grid(alpha=.3)\n",
        "\n",
        "# Adjust layout to prevent overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Transfer Learning\n",
        "\n",
        "<img src=\"https://radekosmulski.com/content/images/2021/08/imagenet_banner.jpeg\" width=\"700\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "snyItJiYZvks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEqu2E0c0moS"
      },
      "outputs": [],
      "source": [
        "# Initialise MobileNetV3Small model with pretrained weights, for transfer learning\n",
        "mobilenet = tfk.applications.MobileNetV3Small(\n",
        "    input_shape=(128, 128, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg',\n",
        "    include_preprocessing=True\n",
        ")\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "mobilenet.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(mobilenet, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVb3EhgE0IuM"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1OYCCe7eQSSiscBZC1gc1TmIMOjN8R-Qk\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VSxNoCw0mij"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers in MobileNetV3Small to use it solely as a feature extractor\n",
        "mobilenet.trainable = False\n",
        "\n",
        "# Define input layer with shape matching the input images\n",
        "inputs = tfk.Input(shape=(128, 128, 3), name='input_layer')\n",
        "\n",
        "# Apply data augmentation for training robustness\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal\"),\n",
        "    tfkl.RandomTranslation(0.2, 0.2)\n",
        "], name='preprocessing')\n",
        "\n",
        "x = augmentation(inputs)\n",
        "\n",
        "# Pass augmented inputs through the MobileNetV3Small feature extractor\n",
        "x = mobilenet(x)\n",
        "\n",
        "# Add a dropout layer for regularisation\n",
        "x = tfkl.Dropout(0.3, name='dropout')(x)\n",
        "\n",
        "# Add final Dense layer for classification with softmax activation\n",
        "outputs = tfkl.Dense(y_train.shape[-1], activation='softmax', name='dense')(x)\n",
        "\n",
        "# Define the complete model linking input and output\n",
        "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss and Adam optimiser\n",
        "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "tl_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(tl_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIsXmmkx0mf5"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "tl_history = tl_model.fit(\n",
        "    x=X_train * 255,\n",
        "    y=y_train,\n",
        "    batch_size=64,\n",
        "    epochs=200,\n",
        "    validation_data=(X_val * 255, y_val),\n",
        "    callbacks=[tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
        ").history\n",
        "\n",
        "# Calculate and print the best validation accuracy achieved\n",
        "final_val_accuracy = round(max(tl_history['val_accuracy']) * 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file, including final accuracy in the filename\n",
        "model_filename = 'Pets_MobileNetV3S_' + str(final_val_accuracy) + '.keras'\n",
        "tl_model.save(model_filename)\n",
        "\n",
        "# Free memory by deleting the model instance\n",
        "del tl_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqXTG5o4cL2L"
      },
      "outputs": [],
      "source": [
        "# Create figure and subplots for loss and accuracy\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))\n",
        "\n",
        "# Plot loss for both re-trained and transfer learning models\n",
        "ax1.plot(history['loss'], alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(history['val_loss'], label='Re-trained', alpha=0.8, color='#ff7f0e')\n",
        "ax1.plot(tl_history['loss'], alpha=0.3, color='#4D61E2', linestyle='--')\n",
        "ax1.plot(tl_history['val_loss'], label='Transfer Learning', alpha=0.8, color='#4D61E2')\n",
        "ax1.set_title('Categorical Crossentropy')\n",
        "ax1.legend(loc='upper left')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot accuracy for both re-trained and transfer learning models\n",
        "ax2.plot(history['accuracy'], alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(history['val_accuracy'], label='Re-trained', alpha=0.8, color='#ff7f0e')\n",
        "ax2.plot(tl_history['accuracy'], alpha=0.3, color='#4D61E2', linestyle='--')\n",
        "ax2.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=0.8, color='#4D61E2')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust layout to prevent label overlap and display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2kfM9NlLJh"
      },
      "source": [
        "## üõ†Ô∏è Fine Tuning\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/359405075/figure/fig2/AS:1182999492214798@1659060466845/Concept-of-fine-tuning-and-feature-extraction_W640.jpg\" width=\"700\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlL1Yb_GIIbr"
      },
      "outputs": [],
      "source": [
        "# Re-load the model after transfer learning\n",
        "ft_model = tfk.models.load_model('Pets_MobileNetV3S_TL_74.43.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q68cKl2W0mXT"
      },
      "outputs": [],
      "source": [
        "# Set the MobileNetV3Small model layers as trainable\n",
        "ft_model.get_layer('MobileNetV3Small').trainable = True\n",
        "\n",
        "# Set all MobileNetV3Small layers as non-trainable\n",
        "for layer in ft_model.get_layer('MobileNetV3Small').layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Enable training only for Conv2D and DepthwiseConv2D layers\n",
        "for i, layer in enumerate(ft_model.get_layer('MobileNetV3Small').layers):\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
        "        layer.trainable = True\n",
        "        print(i, layer.name, type(layer).__name__, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrasXwPsF1zi"
      },
      "outputs": [],
      "source": [
        "# Set the number of layers to freeze\n",
        "N = 124\n",
        "\n",
        "# Set the first N layers as non-trainable\n",
        "for i, layer in enumerate(ft_model.get_layer('MobileNetV3Small').layers[:N]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# Print layer indices, names, and trainability status\n",
        "for i, layer in enumerate(ft_model.get_layer('MobileNetV3Small').layers):\n",
        "    print(i, layer.name, layer.trainable)\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWdPyqwRF1wk"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XveYFnj2F1tz"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "ft_history = ft_model.fit(\n",
        "    x = X_train*255,\n",
        "    y = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 200,\n",
        "    validation_data = (X_val*255, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'Pets_MobileNetV3S_FT_'+str(final_val_accuracy)+'.keras'\n",
        "ft_model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up resources\n",
        "del ft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHNN2DuJtdZS"
      },
      "outputs": [],
      "source": [
        "# Create figure and subplots for loss and accuracy\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))\n",
        "\n",
        "# Plot categorical cross-entropy loss for both Transfer Learning and Fine Tuning stages\n",
        "ax1.plot(tl_history['loss'], alpha=0.3, color='#4D61E2', linestyle='--')\n",
        "ax1.plot(tl_history['val_loss'], label='Transfer Learning', alpha=0.8, color='#4D61E2')\n",
        "ax1.plot(ft_history['loss'], alpha=0.3, color='#408537', linestyle='--')\n",
        "ax1.plot(ft_history['val_loss'], label='Fine Tuning', alpha=0.8, color='#408537')\n",
        "ax1.set_title('Categorical Crossentropy')\n",
        "ax1.legend(loc='upper left')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot accuracy for both Transfer Learning and Fine Tuning stages\n",
        "ax2.plot(tl_history['accuracy'], alpha=0.3, color='#4D61E2', linestyle='--')\n",
        "ax2.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=0.8, color='#4D61E2')\n",
        "ax2.plot(ft_history['accuracy'], alpha=0.3, color='#408537', linestyle='--')\n",
        "ax2.plot(ft_history['val_accuracy'], label='Fine Tuning', alpha=0.8, color='#408537')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.set_ylim([min(tl_history['val_accuracy']) * 0.95, max(ft_history['val_accuracy']) * 1.05])\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust layout to prevent overlap and display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tigJr33nE43"
      },
      "source": [
        "## üïπÔ∏è Use the Model - Make Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned MobileNetV3 model for prediction on external images\n",
        "ft_bn_model = tfk.models.load_model('Pets_MobileNetV3S_FT_76.94.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "ft_model.summary(expand_nested=True)\n",
        "\n",
        "# Display model architecture with layer shapes and trainable parameters\n",
        "tfk.utils.plot_model(ft_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "mJmO2pCxg3iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCCDJMttF1nv"
      },
      "outputs": [],
      "source": [
        "def crop_square(image):\n",
        "    \"\"\"Crop image to a square, centred on the middle section\"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    height = tf.shape(image)[0]\n",
        "    width = tf.shape(image)[1]\n",
        "    crop_size = tf.minimum(height, width)\n",
        "    height_offset = (height - crop_size) // 2\n",
        "    width_offset = (width - crop_size) // 2\n",
        "\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image,\n",
        "        height_offset,\n",
        "        width_offset,\n",
        "        crop_size,\n",
        "        crop_size\n",
        "    )\n",
        "    return image\n",
        "\n",
        "def preprocess_for_model(image):\n",
        "    \"\"\"Resize image to match model input requirements\"\"\"\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    return image\n",
        "\n",
        "# Dictionary of image URLs and their actual classes\n",
        "images = {\n",
        "    'pug': 'https://blog.expodog.com/wp-content/uploads/2020/09/f896a647cfc5346c5b042a6a1e916065.jpg',\n",
        "    'persian': 'https://images.squarespace-cdn.com/content/v1/5b1cc0f95b409bd4bfc3b316/1687322332437-DOVUPJBJMKQWZ2TRSQA2/sergey-semin-I9cHfDYLT3E-unsplash%281%29.jpg',\n",
        "    'sphynx': 'https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcpzLYLRORZQaVRNizLejwVBFp8eIUPS_ZN7Zhw_J54RgeELoupne4wHwxj8o8vr6XXlFtJwTj8QGP7VIpbj_UwTU6bp4Z6lIIM80zkyhKBtN8YHeFQQScopDh0nw8loMvhmxvaurUuP8/s1600/Sphynx_associazione_mammagatta-7.jpg',\n",
        "    'abyssinian': 'https://www.thesprucepets.com/thmb/MigiLYeVSJcj0zkNVkJ--rALMZI=/2121x0/filters:no_upscale():strip_icc()/GettyImages-165827729-efc11c02690f457a81ef6ccbfa8eb34d.jpg',\n",
        "    'samoyed': 'https://images.ctfassets.net/440y9b545yd9/49v1AZmZdiPYkJ4A3vrayj/d7d7db21fed2ef30f5b8e3899633d292/Samoyed850.jpg'\n",
        "}\n",
        "\n",
        "# Create a plot for visualising model predictions\n",
        "plt.figure(figsize=(25, 5))\n",
        "\n",
        "for i, (true_class, url) in enumerate(images.items(), 1):\n",
        "    # Load the image from the URL\n",
        "    response = requests.get(url)\n",
        "    img_original = np.array(Image.open(BytesIO(response.content))) / 255\n",
        "\n",
        "    # Crop the image to a square for consistent processing\n",
        "    img_square = crop_square(img_original)\n",
        "\n",
        "    # Preprocess image copy for model prediction\n",
        "    img_for_model = preprocess_for_model(img_square)\n",
        "\n",
        "    # Predict class using preprocessed image\n",
        "    prediction = ft_bn_model.predict(np.expand_dims(img_for_model * 255, axis=0), verbose=0)\n",
        "    pred_class = class_names[np.argmax(prediction)]\n",
        "    confidence = round(100 * np.max(prediction), 2)\n",
        "\n",
        "    # Plot the cropped square image\n",
        "    plt.subplot(1, 5, i)\n",
        "    plt.imshow(img_square)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add title with actual and predicted class info\n",
        "    plt.title(f'Real: {true_class}\\nPred: {pred_class}\\nConf: {confidence}%', fontsize=14, pad=10)\n",
        "\n",
        "# Adjust layout to prevent overlap and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4tWF6oUQFqH"
      },
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"15\"> **Instagram:** https://www.instagram.com/airlab_polimi/\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"15\"> **LinkedIn:** https://www.linkedin.com/company/airlab-polimi/\n",
        "___\n",
        "Credits: Eugenio Lomurno üìß eugenio.lomurno@polimi.it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "   Copyright 2024 Eugenio Lomurno\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}