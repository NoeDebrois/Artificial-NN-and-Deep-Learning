{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoeDebrois/Artificial-NN-and-Deep-Learning/blob/main/Object_Localisation_and_Class_Activation_Maps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Lecture 5a: Object Localisation and Class Activation Maps\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14qXmXmQHVwDxXJ3DiVhNmMOcnpA6QMiq\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "rhp45709H7JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üåê Connect Colab to Google Drive"
      ],
      "metadata": {
        "id": "PlBIn2o3uleJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "764pDku-udyp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2024-2025] AN2DL/Lecture 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Import Libraries"
      ],
      "metadata": {
        "id": "p8I_2uDtu19o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import additional libraries\n",
        "import cv2\n",
        "import csv\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from xml.dom import minidom\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "uYcdpZSjunyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è≥ Load and Process Data"
      ],
      "metadata": {
        "id": "nLDHj3vfv0O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables for training dataset\n",
        "os.environ[\"TRAINING_DATASET_NAME\"] = \"cats_dogs_images_train.zip\"\n",
        "os.environ[\"TRAINING_DATASET_URL\"] = \"1_fGNrYZxs0yzIJQfUmUWHrWnRisVEYaY\"\n",
        "\n",
        "# Check if training dataset exists, download if not\n",
        "if not os.path.exists(os.environ[\"TRAINING_DATASET_NAME\"]):\n",
        "    print(\"Training data downloading...\")\n",
        "    ! gdown -q ${TRAINING_DATASET_URL}\n",
        "    ! unzip ${TRAINING_DATASET_NAME}\n",
        "    print(\"Training data downloaded!\")\n",
        "else:\n",
        "    print(\"Training data already downloaded, using cached data.\")\n",
        "\n",
        "# Set environment variables for bounding boxes of training dataset\n",
        "os.environ[\"TRAINING_DATASET_BOUNDING_BOXES_NAME\"] = \"cats_dogs_images_boxes.csv\"\n",
        "os.environ[\"TRAINING_DATASET_BOUNDING_BOXES_URL\"] = \"1visBcJA_F9oUOAOTNq6R-MTzkFBXa2LY\"\n",
        "\n",
        "# Check if bounding boxes file exists, download if not\n",
        "if not os.path.exists(os.environ[\"TRAINING_DATASET_BOUNDING_BOXES_NAME\"]):\n",
        "    print(\"Training data bounding boxes downloading...\")\n",
        "    ! gdown -q ${TRAINING_DATASET_BOUNDING_BOXES_URL}\n",
        "    print(\"Training data bounding boxes downloaded!\")\n",
        "else:\n",
        "    print(\"Training data bounding boxes already downloaded, using cached data.\")\n",
        "\n",
        "# Set environment variables for test dataset\n",
        "os.environ[\"TEST_DATASET_NAME\"] = \"cats_dogs_images_test.zip\"\n",
        "os.environ[\"TEST_DATASET_URL\"] = \"1RFJwHLkLdj3RVq-xkYtP_8uLkj5K-obn\"\n",
        "\n",
        "# Check if test dataset exists, download if not\n",
        "if not os.path.exists(os.environ[\"TEST_DATASET_NAME\"]):\n",
        "    print(\"Test data downloading...\")\n",
        "    ! gdown -q ${TEST_DATASET_URL}\n",
        "    ! unzip ${TEST_DATASET_NAME}\n",
        "    print(\"Test data downloaded!\")\n",
        "else:\n",
        "    print(\"Test data already downloaded, using cached data.\")"
      ],
      "metadata": {
        "id": "PMkiC1wWd1GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder, img_dim):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder,filename))\n",
        "\n",
        "        # Make the image dataset squared\n",
        "        dim = min(img.shape[:-1])\n",
        "        img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2,(img.shape[1]-dim)//2:(img.shape[1]+dim)//2,:]\n",
        "\n",
        "        # Resize all images to a fix size\n",
        "        img = cv2.resize(img, (img_dim, img_dim))\n",
        "\n",
        "        # Convert the image from BGR to RGB as NasNetMobile was trained on RGB images\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "kpJ_sZTS2Qx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(image_dir='cats_dogs_images', image_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Preprocesses a dataset and returns the labels, boxes, and images as a tuple.\n",
        "\n",
        "    Args:\n",
        "        image_dir (str): The directory containing the images in the dataset.\n",
        "        image_size (tuple): The size of the images in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the labels, boxes, and images.\n",
        "    \"\"\"\n",
        "    # Read the CSV file with the dataset\n",
        "    df = pd.read_csv('cats_dogs_images_boxes.csv')\n",
        "\n",
        "    # Define a function to process each row\n",
        "    def process_row(row):\n",
        "        img_path = row[0]\n",
        "        label = int(row[3])\n",
        "        bbox_coords = list(map(float, row[4:8]))\n",
        "\n",
        "        # Read the image using OpenCV\n",
        "        img = cv2.imread(os.path.join(image_dir, img_path))\n",
        "        if img is None:\n",
        "            return None  # Skip if image is not found\n",
        "\n",
        "        # Get original image dimensions\n",
        "        orig_height, orig_width = img.shape[:2]\n",
        "\n",
        "        # Scale the bounding box coordinates using original dimensions\n",
        "        x1, y1, x2, y2 = bbox_coords\n",
        "        x1 /= orig_width\n",
        "        y1 /= orig_height\n",
        "        x2 /= orig_width\n",
        "        y2 /= orig_height\n",
        "        bbox = [x1, y1, x2, y2]\n",
        "\n",
        "        # Resize the image\n",
        "        img_resized = cv2.resize(img, image_size)\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        return label, bbox, img_rgb\n",
        "\n",
        "    # Use ThreadPoolExecutor to process images in parallel\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = list(executor.map(process_row, df.itertuples(index=False)))\n",
        "\n",
        "    # Filter out any None results\n",
        "    results = [r for r in results if r is not None]\n",
        "\n",
        "    # Unzip the results\n",
        "    labels, boxes, img_list = zip(*results)\n",
        "\n",
        "    # Convert to lists\n",
        "    return list(labels), list(boxes), list(img_list)"
      ],
      "metadata": {
        "id": "mh5DCAHo5xYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get augmented images and corresponding bounding boxes and labels\n",
        "labels, boxes, img_list = preprocess_dataset()\n",
        "\n",
        "# Shuffle the data by zipping the lists and shuffling the combined list\n",
        "combined_list = list(zip(img_list, boxes, labels))\n",
        "random.shuffle(combined_list)\n",
        "\n",
        "# Unpack the shuffled lists\n",
        "img_list, boxes, labels = zip(*combined_list)\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "img_list, boxes, labels = np.array(img_list), np.array(boxes), np.array(labels)\n",
        "\n",
        "num_to_labels = {0: 'cat', 1: 'dog'}\n",
        "\n",
        "# Print the shape of each list\n",
        "print(f\"Image list shape: {img_list.shape}\")\n",
        "print(f\"Bounding boxes shape: {boxes.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "YIR_fTVqwsfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the image size for resizing\n",
        "img_size = 256\n",
        "\n",
        "# Create a Matplotlib figure with a fixed size\n",
        "fig, ax = plt.subplots(figsize=(20, 20), facecolor='white')\n",
        "\n",
        "# Generate a random sample of indices from the image list\n",
        "random_indices = random.sample(range(len(img_list)), 20)\n",
        "\n",
        "# Iterate over the random indices and plot the corresponding images\n",
        "for i, index in enumerate(random_indices, 1):\n",
        "\n",
        "    # Extract the bounding box coordinates for the current image\n",
        "    x1, y1, x2, y2 = boxes[index]\n",
        "\n",
        "    # Rescale the bounding box coordinates to match the image size\n",
        "    x1, y1, x2, y2 = x1 * img_size, y1 * img_size, x2 * img_size, y2 * img_size\n",
        "\n",
        "    # Retrieve the current image and clip its values to the range 0-255\n",
        "    image = img_list[index]\n",
        "    image = np.clip(image, 0, 255)\n",
        "\n",
        "    # Draw a green bounding box on the image\n",
        "    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 3)\n",
        "\n",
        "    # Plot the image on a subplot and turn off the axis labels\n",
        "    ax = plt.subplot(4, 5, i)\n",
        "    ax.imshow(image)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Adjust the spacing between the subplots to improve readability\n",
        "plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "# Display the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lAhkTVl3xAh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-test split on images, labels, and bounding boxes\n",
        "train_images, val_images, train_labels, val_labels, train_boxes, val_boxes = train_test_split(\n",
        "    img_list, labels, boxes, test_size=0.2, random_state=seed, stratify=labels\n",
        ")\n",
        "\n",
        "# Output the number of elements in the training and validation sets for verification\n",
        "print('Number of training images: ', train_images.shape[0])\n",
        "print('Number of training labels: ', train_labels.shape[0])\n",
        "print('Number of training boxes: ', train_boxes.shape[0])\n",
        "print('Number of validation images: ', val_images.shape[0])\n",
        "print('Number of validation labels: ', val_labels.shape[0])\n",
        "print('Number of validation boxes: ', val_boxes.shape[0])\n",
        "\n",
        "# Convert labels to one-hot encoded format for use in training\n",
        "train_labels = tfk.utils.to_categorical(train_labels, num_classes=2)\n",
        "val_labels = tfk.utils.to_categorical(val_labels, num_classes=2)"
      ],
      "metadata": {
        "id": "ZbnkNSHVxvDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=15T4O0D_r2AF3M1FzHaqf1z2y5NXV43t2\" width=\"900\"/>\n"
      ],
      "metadata": {
        "id": "xM_Fdb6GTrx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üõ†Ô∏è Models and Experiments"
      ],
      "metadata": {
        "id": "x9nOJ9vFyA1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spearman_rho(box_predictions, val_boxes):\n",
        "\n",
        "    # Reshape predictions and validation boxes into 1-D tensors\n",
        "    box_predictions = tf.reshape(box_predictions, [-1])\n",
        "    val_boxes = tf.reshape(val_boxes, [-1])\n",
        "\n",
        "    # Function to compute ranks of elements\n",
        "    def rank(x):\n",
        "        # Sort elements and obtain their indices\n",
        "        sorted_indices = tf.argsort(x, direction='ASCENDING')\n",
        "\n",
        "        # Assign ranks based on the sorted indices\n",
        "        ranks = tf.argsort(sorted_indices, direction='ASCENDING') + 1\n",
        "\n",
        "        return tf.cast(ranks, tf.float32)\n",
        "\n",
        "    # Compute ranks for the predicted and actual boxes\n",
        "    rank_pred = rank(box_predictions)\n",
        "    rank_val = rank(val_boxes)\n",
        "\n",
        "    # Calculate the mean of the ranks\n",
        "    mean_rank_pred = tf.reduce_mean(rank_pred)\n",
        "    mean_rank_val = tf.reduce_mean(rank_val)\n",
        "\n",
        "    # Calculate differences from the mean ranks\n",
        "    diff_pred = rank_pred - mean_rank_pred\n",
        "    diff_val = rank_val - mean_rank_val\n",
        "\n",
        "    # Compute covariance of the rank differences\n",
        "    cov = tf.reduce_mean(diff_pred * diff_val)\n",
        "\n",
        "    # Compute standard deviations of the rank differences\n",
        "    std_pred = tf.sqrt(tf.reduce_mean(tf.square(diff_pred)))\n",
        "    std_val = tf.sqrt(tf.reduce_mean(tf.square(diff_val)))\n",
        "\n",
        "    # Compute Spearman's rank correlation coefficient with epsilon to avoid division by zero\n",
        "    spearman_rho = cov / (std_pred * std_val + 1e-8)\n",
        "\n",
        "    return spearman_rho"
      ],
      "metadata": {
        "id": "jp8fj5z-rhpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Multitask Learning"
      ],
      "metadata": {
        "id": "2RT3sDvDyByx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained MobileNetV2 model without the top layer,\n",
        "# using 'imagenet' weights, and configure it for transfer learning\n",
        "mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape = train_images.shape[1:],\n",
        "    include_top = False,\n",
        "    weights = 'imagenet',\n",
        "    pooling = 'avg',\n",
        ")\n",
        "\n",
        "# Plot the architecture of the MobileNetV2 model, displaying trainable parameters and the shapes of each layer\n",
        "tfk.utils.plot_model(mobile, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "sG3oILfVyGkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze MobileNet layers\n",
        "mobile.trainable = False\n",
        "\n",
        "# Define inputs and add MobileNet as a feature extractor\n",
        "inputs = tfk.Input(shape=train_images.shape[1:])\n",
        "x = mobile(inputs)\n",
        "\n",
        "# Add dropout layer and classifier head for object classification\n",
        "x = tfkl.Dropout(0.5, seed=seed)(x)\n",
        "class_outputs = tfkl.Dense(2, activation='softmax', name='classifier')(x)\n",
        "\n",
        "# Add localisation head for bounding box prediction\n",
        "box_outputs = tfkl.Dense(4, activation='linear', name='localizer')(x)\n",
        "\n",
        "# Create the model connecting inputs to classification and localisation outputs\n",
        "object_localization_model = tfk.Model(inputs=inputs, outputs=[class_outputs, box_outputs], name='object_localization_model')\n",
        "\n",
        "# Compile the model with categorical crossentropy and mean squared error losses, using Adam optimiser\n",
        "object_localization_model.compile(loss=[tfk.losses.CategoricalCrossentropy(), tfk.losses.MeanSquaredError()], optimizer=tfk.optimizers.Adam())\n",
        "\n",
        "# Display model summary and plot the model architecture\n",
        "object_localization_model.summary()\n",
        "tfk.utils.plot_model(object_localization_model, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "0irBEF2PyGhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training with input images and associated labels and bounding boxes\n",
        "object_localization_history = object_localization_model.fit(\n",
        "    x = preprocess_input(train_images),\n",
        "    y = [train_labels, train_boxes],\n",
        "    batch_size = 64,\n",
        "    epochs = 200,\n",
        "\n",
        "    # Validation data provided for performance monitoring\n",
        "    validation_data = (preprocess_input(val_images), [val_labels, val_boxes]),\n",
        "\n",
        "    # Callbacks for early stopping and learning rate reduction based on validation loss\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_delta=1e-5)\n",
        "    ]\n",
        ").history"
      ],
      "metadata": {
        "id": "Apb3gdRGyGez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "object_localization_model.save('MultitaskCatDogLocalizer.keras')\n",
        "del object_localization_model"
      ],
      "metadata": {
        "id": "dFWFQahyyGUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the model after transfer learning\n",
        "object_localization_model = tfk.models.load_model('MultitaskCatDogLocalizer.keras')\n",
        "object_localization_model.summary()"
      ],
      "metadata": {
        "id": "7AEEH_si0fB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the validation images\n",
        "val_predictions = object_localization_model.predict(preprocess_input(val_images), verbose=0)\n",
        "classification_predictions = np.argmax(val_predictions[0], axis=1)\n",
        "box_predictions = val_predictions[1]\n",
        "\n",
        "# Retrieve true labels from the validation set\n",
        "val_gt = np.argmax(val_labels, axis=1)\n",
        "\n",
        "# Compute and display confusion matrix\n",
        "cm = confusion_matrix(val_gt, classification_predictions)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm.T, fmt='d', xticklabels=list(num_to_labels.values()),\n",
        "            yticklabels=list(num_to_labels.values()), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute and display accuracy on the validation set\n",
        "val_accuracy = accuracy_score(val_gt, classification_predictions)\n",
        "print(f'Accuracy score on validation set: {round(val_accuracy*100, 2)}')\n",
        "\n",
        "# Compute and display precision on the validation set\n",
        "val_precision = precision_score(val_gt, classification_predictions, average='macro')\n",
        "print(f'Precision score on validation set: {round(val_precision*100, 2)}')\n",
        "\n",
        "# Compute and display recall on the validation set\n",
        "val_recall = recall_score(val_gt, classification_predictions, average='macro')\n",
        "print(f'Recall score on validation set: {round(val_recall*100, 2)}')\n",
        "\n",
        "# Compute and display F1 score on the validation set\n",
        "val_f1 = f1_score(val_gt, classification_predictions, average='macro')\n",
        "print(f'F1 score on validation set: {round(val_f1*100, 2)}')\n",
        "\n",
        "# Compute and display Spearman's Rho correlation for bounding box predictions\n",
        "spearman = spearman_rho(val_boxes, box_predictions)\n",
        "print(f'Spearman\\'s Rho Correlation on validation set: {round(float(spearman),4)}')"
      ],
      "metadata": {
        "id": "KdBa4eocit4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üïπÔ∏è Use the Model - Make Inference"
      ],
      "metadata": {
        "id": "JUDZAhYIR3YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test images from the specified folder path with a target size of 256\n",
        "test_path = 'localization_test/'\n",
        "X_test = load_images_from_folder(test_path, 256)"
      ],
      "metadata": {
        "id": "BI-q2vHq2Uwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of 10 images from the test data\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i % 2, i % (num_img // 2)]\n",
        "    ax.imshow(np.clip(X_test[i], 0, 255))\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RhgIpymQ2erZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the test images\n",
        "test_predictions = object_localization_model.predict(preprocess_input(X_test), verbose=0)"
      ],
      "metadata": {
        "id": "5uPJp0nZ2tv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of 10 test images with predicted bounding boxes and class labels\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "\n",
        "for i in range(num_img):\n",
        "    row = i // (num_img // 2)\n",
        "    col = i % (num_img // 2)\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    # Extract bounding box predictions\n",
        "    a1, b1, a2, b2 = test_predictions[1][i]\n",
        "    img_size = 256\n",
        "\n",
        "    # Rescale bounding box values to match image size\n",
        "    x1 = a1 * img_size\n",
        "    y1 = b1 * img_size\n",
        "    x2 = a2 * img_size\n",
        "    y2 = b2 * img_size\n",
        "\n",
        "    img = X_test[i].copy()\n",
        "\n",
        "    # Draw bounding boxes on the image\n",
        "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 3)\n",
        "\n",
        "    # Get predicted class probabilities and label\n",
        "    probabilities = test_predictions[0][i]\n",
        "    predicted_class = np.argmax(probabilities)\n",
        "    confidence = round(probabilities[predicted_class] * 100, 1)\n",
        "    label = 'dog' if predicted_class == 1 else 'cat'\n",
        "\n",
        "    # Display image with bounding box and label\n",
        "    ax.imshow(np.clip(img, 0, 255).astype(np.uint8))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'{label}: {confidence}%')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4pbl1QnCNiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Double Neural Network"
      ],
      "metadata": {
        "id": "1pN0mOzN3xRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Classifier**"
      ],
      "metadata": {
        "id": "PZKKCE1h33t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape = train_images.shape[1:],\n",
        "    include_top = False,\n",
        "    weights = 'imagenet',\n",
        "    pooling = 'avg',\n",
        ")"
      ],
      "metadata": {
        "id": "8Jbl_7n633CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use MobileNet as feature extractor\n",
        "mobile.trainable = False\n",
        "\n",
        "# Add the classifier to MobileNet\n",
        "inputs = tfk.Input(shape=train_images.shape[1:])\n",
        "x = mobile(inputs)\n",
        "x = tfkl.Dropout(0.5, seed=seed)(x)\n",
        "outputs = tfkl.Dense(2, activation='softmax', name='classifier')(x)\n",
        "\n",
        "# Connect input and output through the Model class\n",
        "classifier_model = tfk.Model(inputs=inputs, outputs=outputs, name='classifier_model')\n",
        "\n",
        "# Compile the model\n",
        "classifier_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
        "classifier_model.summary()"
      ],
      "metadata": {
        "id": "Tb1kk5JE39go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "classifier_history = classifier_model.fit(\n",
        "    x = preprocess_input(train_images),\n",
        "    y = train_labels,\n",
        "    batch_size = 64,\n",
        "    epochs = 200,\n",
        "    validation_data = (preprocess_input(val_images), val_labels),\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=15, min_delta=1e-5)\n",
        "    ]\n",
        ").history"
      ],
      "metadata": {
        "id": "rn4bR5yK4VSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "classifier_model.save('CatDogClassifier.keras')\n",
        "del classifier_model"
      ],
      "metadata": {
        "id": "V2hYm8ms4k-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the model after transfer learning\n",
        "classifier_model = tfk.models.load_model('CatDogClassifier.keras')\n",
        "classifier_model.summary()"
      ],
      "metadata": {
        "id": "VZYpFZu85Fe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the validation images\n",
        "val_predictions = classifier_model.predict(preprocess_input(val_images), verbose=0)\n",
        "classification_predictions = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Retrieve true labels from the validation set\n",
        "val_gt = np.argmax(val_labels, axis=1)\n",
        "\n",
        "# Compute and display confusion matrix\n",
        "cm = confusion_matrix(val_gt, classification_predictions)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm.T, fmt='d', xticklabels=list(num_to_labels.values()),\n",
        "            yticklabels=list(num_to_labels.values()), cmap='Blues', annot=True)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Compute and display accuracy on the validation set\n",
        "val_accuracy = accuracy_score(val_gt, classification_predictions)\n",
        "print(f'Accuracy score on validation set: {round(val_accuracy*100, 2)}')\n",
        "\n",
        "# Compute and display precision on the validation set\n",
        "val_precision = precision_score(val_gt, classification_predictions, average='macro')\n",
        "print(f'Precision score on validation set: {round(val_precision*100, 2)}')\n",
        "\n",
        "# Compute and display recall on the validation set\n",
        "val_recall = recall_score(val_gt, classification_predictions, average='macro')\n",
        "print(f'Recall score on validation set: {round(val_recall*100, 2)}')\n",
        "\n",
        "# Compute and display F1 score on the validation set\n",
        "val_f1 = f1_score(val_gt, classification_predictions, average='macro')\n",
        "print(f'F1 score on validation set: {round(val_f1*100, 2)}')"
      ],
      "metadata": {
        "id": "hhnUbxGeDKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Box Regressor**"
      ],
      "metadata": {
        "id": "469Ea9p25e3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape = train_images.shape[1:],\n",
        "    include_top = False,\n",
        "    weights = 'imagenet',\n",
        "    pooling = 'avg',\n",
        ")"
      ],
      "metadata": {
        "id": "8O7rD4lg5mjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use MobileNet as feature extractor\n",
        "mobile.trainable = False\n",
        "\n",
        "# Add the classifier to MobileNet\n",
        "inputs = tfk.Input(shape=train_images.shape[1:])\n",
        "x = mobile(inputs)\n",
        "x = tfkl.Dropout(0.5, seed=seed)(x)\n",
        "outputs = tfkl.Dense(4, activation='linear', name='box_regressor')(x)\n",
        "\n",
        "# Connect input and output through the Model class\n",
        "box_regressor_model = tfk.Model(inputs=inputs, outputs=outputs, name='box_regressor_model')\n",
        "\n",
        "# Compile the model\n",
        "box_regressor_model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam())\n",
        "box_regressor_model.summary()"
      ],
      "metadata": {
        "id": "5GvRFvgr5mhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "regressor_history = box_regressor_model.fit(\n",
        "    x = preprocess_input(train_images),\n",
        "    y = train_boxes,\n",
        "    batch_size = 64,\n",
        "    epochs = 200,\n",
        "    validation_data = (preprocess_input(val_images), val_boxes),\n",
        "    callbacks = [\n",
        "            tfk.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "            tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_delta=1e-5)\n",
        "    ]\n",
        ").history"
      ],
      "metadata": {
        "id": "izpvQblq5mek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "box_regressor_model.save('CatDogBoxRegressor.keras')\n",
        "del box_regressor_model"
      ],
      "metadata": {
        "id": "OwuVb2yT-F_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the model after transfer learning\n",
        "box_regressor_model = tfk.models.load_model('CatDogBoxRegressor.keras')\n",
        "box_regressor_model.summary()"
      ],
      "metadata": {
        "id": "5_B6Z4-U-Ie-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for bounding box regression on validation images\n",
        "predictions = box_regressor_model.predict(preprocess_input(val_images), verbose=0)\n",
        "\n",
        "# Compute and display Spearman's Rho correlation between true and predicted bounding boxes\n",
        "spearman = spearman_rho(val_boxes, predictions)\n",
        "print(f\"Spearman's Rho Correlation on validation set: {round(float(spearman), 4)}\")"
      ],
      "metadata": {
        "id": "s9_ofrGd-DIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üïπÔ∏è Use the Model - Make Inference"
      ],
      "metadata": {
        "id": "RpMeXDTX9MyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test images from the specified folder with the given image size\n",
        "test_path = 'localization_test/'\n",
        "X_test = load_images_from_folder(test_path, img_size)"
      ],
      "metadata": {
        "id": "quQ_DwyP5mcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of 10 test images\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "\n",
        "for i in range(num_img):\n",
        "    ax = axes[i % 2, i % (num_img // 2)]\n",
        "    ax.imshow(np.clip(X_test[i], 0, 255))\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWjfage25mZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate class predictions and bounding box predictions on the test images\n",
        "test_class_predictions = classifier_model.predict(preprocess_input(X_test), verbose=0)\n",
        "test_box_predictions = box_regressor_model.predict(preprocess_input(X_test), verbose=0)"
      ],
      "metadata": {
        "id": "VjSnh-f79Umx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the predictions\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "\n",
        "for i in range(num_img):\n",
        "    row = i // (num_img // 2)\n",
        "    col = i % (num_img // 2)\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    # Bounding box of each image\n",
        "    a1, b1, a2, b2 = test_box_predictions[i]\n",
        "    img_size = 256\n",
        "\n",
        "    # Rescaling the bounding box values to match the image size\n",
        "    x1 = a1 * img_size\n",
        "    y1 = b1 * img_size\n",
        "    x2 = a2 * img_size\n",
        "    y2 = b2 * img_size\n",
        "\n",
        "    img = X_test[i].copy()\n",
        "\n",
        "    # Draw bounding boxes on the image\n",
        "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 3)\n",
        "\n",
        "    # Get predicted probabilities\n",
        "    probabilities = test_class_predictions[i]\n",
        "    predicted_class = np.argmax(probabilities)\n",
        "    confidence = round(probabilities[predicted_class] * 100, 1)\n",
        "    label = 'dog' if predicted_class == 1 else 'cat'\n",
        "\n",
        "    ax.imshow(np.clip(img, 0, 255).astype(np.uint8))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'{label}: {confidence}%')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sY1tfnwuFPba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üëÅÔ∏è Class Activation Maps"
      ],
      "metadata": {
        "id": "MoTeT7Wj_P5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test images from the specified folder and preprocess them\n",
        "test_path = 'localization_test'\n",
        "X_test = load_images_from_folder(test_path, img_size)\n",
        "X_test_preprocessed = preprocess_input(X_test)"
      ],
      "metadata": {
        "id": "iOP87BUC_TgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_CAM(model, img):\n",
        "    # Expand image dimensions to fit the model input shape\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Predict to get the winning class\n",
        "    predictions = model.predict(img, verbose=0)\n",
        "    label_index = np.argmax(predictions)\n",
        "\n",
        "    # Get the 1028 input weights to the softmax of the winning class\n",
        "    class_weights = model.layers[-1].get_weights()[0]\n",
        "    class_weights_winner = class_weights[:, label_index]\n",
        "\n",
        "    # Define the final convolutional layer of the MobileNetV2 model\n",
        "    final_conv_layer = tfk.Model(\n",
        "        model.get_layer('mobilenetv2_1.00_224').input,\n",
        "        model.get_layer('mobilenetv2_1.00_224').get_layer('Conv_1').output\n",
        "    )\n",
        "\n",
        "    # Compute the convolutional outputs and squeeze the dimensions\n",
        "    conv_outputs = final_conv_layer(img)\n",
        "    conv_outputs = np.squeeze(conv_outputs)\n",
        "\n",
        "    # Upsample the convolutional outputs and compute the final output using the class weights\n",
        "    mat_for_mult = scipy.ndimage.zoom(conv_outputs, (32, 32, 1), order=1)\n",
        "    final_output = np.dot(mat_for_mult.reshape((256*256, 1280)), class_weights_winner).reshape(256,256)\n",
        "\n",
        "    return final_output, label_index, predictions"
      ],
      "metadata": {
        "id": "JKXSxJvN_crr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Class Activation Map (CAM) values for each image in the preprocessed test set\n",
        "values = []\n",
        "for img in X_test_preprocessed:\n",
        "    values.append(compute_CAM(classifier_model, img))"
      ],
      "metadata": {
        "id": "uC8yhJcTAMLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(2, num_img//2, figsize=(20,9))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i%2,i%num_img//2]\n",
        "    ax.imshow(values[i][0], cmap='turbo')\n",
        "    ax.imshow(np.clip(X_test[i], 0, 255), alpha=0.5)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N05YICHnAcwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample of 10 test images with heatmap-based bounding boxes and class labels\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(2, num_img // 2, figsize=(20, 9))\n",
        "\n",
        "for i in range(num_img):\n",
        "\n",
        "    # Extract the maximum value from the heatmap\n",
        "    heatmap_max = np.max(values[i][0])\n",
        "\n",
        "    # Define a threshold to filter heatmap values\n",
        "    boundary = heatmap_max * 0.3\n",
        "\n",
        "    # Apply the threshold to create a binary heatmap\n",
        "    bbox_heatmap = values[i][0].copy()\n",
        "    bbox_heatmap = np.where(bbox_heatmap <= boundary, 0, 255)\n",
        "\n",
        "    bbox_img = X_test[i].copy()\n",
        "\n",
        "    # Find contours of the heatmap\n",
        "    cnts = cv2.findContours(bbox_heatmap.astype('uint8'),\n",
        "                            cv2.RETR_EXTERNAL,\n",
        "                            cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "\n",
        "    # Define an offset for the bounding box\n",
        "    offset = 10\n",
        "    for c in cnts:\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "        cv2.rectangle(bbox_img, (x + offset, y + offset),\n",
        "                      (x + offset + w, y + h), (100, 255, 0), 3)\n",
        "\n",
        "    # Get the predicted label and confidence score\n",
        "    label = num_to_labels[values[i][1]]\n",
        "    confidence = round(values[i][2][0][values[i][1]] * 100, 1)\n",
        "\n",
        "    # Display image with bounding box and label\n",
        "    ax = axes[i % 2, i % (num_img // 2)]\n",
        "    ax.imshow(np.clip(bbox_img, 0, 255))\n",
        "    ax.title.set_text(f'{label}: {confidence}%')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F5Al7epGBGmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"15\"> **Instagram:** https://www.instagram.com/airlab_polimi/\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"15\"> **LinkedIn:** https://www.linkedin.com/company/airlab-polimi/\n",
        "___\n",
        "Credits: Eugenio Lomurno üìß eugenio.lomurno@polimi.it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "   Copyright 2024 Eugenio Lomurno\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```\n"
      ],
      "metadata": {
        "id": "5sPA7D5dsbS6"
      }
    }
  ]
}