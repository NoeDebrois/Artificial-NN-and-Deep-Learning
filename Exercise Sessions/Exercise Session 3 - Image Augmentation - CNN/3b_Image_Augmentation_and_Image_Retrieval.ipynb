{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoeDebrois/Artificial-NN-and-Deep-Learning/blob/main/3b_Image_Augmentation_and_Image_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Lecture 3b: Image Augmentation and Image Retrieval\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1idXlqipXxn5yDono2mWxxZ94TxSkjiZ4\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "aqy1POvMYG5w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSLbdLvhDRx"
      },
      "source": [
        "### üåê Connect Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoaLQpvChLpb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/Colab Notebooks/Artificial NN & Deep Learning/Exercise Session 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdD_8Vyswkwf"
      },
      "source": [
        "### ‚öôÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_S1JfaW8bIN"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import cv2\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from PIL import Image\n",
        "import matplotlib.gridspec as gridspec\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚è≥ Load Data"
      ],
      "metadata": {
        "id": "_EKV-sJangcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables for Animals dataset\n",
        "os.environ[\"ANIMALS_DATASET_NAME\"] = \"animals.zip\"\n",
        "os.environ[\"ANIMALS_DATASET_URL\"] = \"1nlTR-mwPLc05vxaOncUhXu82NR8gbx63\"\n",
        "\n",
        "# Check if Animals dataset exists, download and unzip if not\n",
        "if not os.path.exists(os.environ[\"ANIMALS_DATASET_NAME\"]):\n",
        "    print(\"Downloading Animals dataset...\")\n",
        "    !gdown -q ${ANIMALS_DATASET_URL} -O ${ANIMALS_DATASET_NAME}\n",
        "    print(\"Animals dataset downloaded!\")\n",
        "\n",
        "    print(\"Unzipping Animals dataset...\")\n",
        "    !unzip -o ${ANIMALS_DATASET_NAME} -d animals\n",
        "    print(\"Animals dataset unzipped!\")\n",
        "else:\n",
        "    print(\"Animals dataset already downloaded and unzipped. Using cached data.\")\n",
        "\n",
        "# Set environment variables for Items dataset\n",
        "os.environ[\"ITEMS_DATASET_NAME\"] = \"items.zip\"\n",
        "os.environ[\"ITEMS_DATASET_URL\"] = \"1tcDVgQYuMnISgFCjaxinXSryB0CAZYHP\"\n",
        "\n",
        "# Check if Items dataset exists, download and unzip if not\n",
        "if not os.path.exists(os.environ[\"ITEMS_DATASET_NAME\"]):\n",
        "    print(\"Downloading Items dataset...\")\n",
        "    !gdown -q ${ITEMS_DATASET_URL} -O ${ITEMS_DATASET_NAME}\n",
        "    print(\"Items dataset downloaded!\")\n",
        "\n",
        "    print(\"Unzipping Items dataset...\")\n",
        "    !unzip -o ${ITEMS_DATASET_NAME} -d items\n",
        "    print(\"Items dataset unzipped!\")\n",
        "else:\n",
        "    print(\"Items dataset already downloaded and unzipped. Using cached data.\")"
      ],
      "metadata": {
        "id": "crdSwKD_6XeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "\n",
        "    # Iterate through files in the specified folder\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder, filename))\n",
        "\n",
        "        # Normalize image pixel values to a float range [0, 1]\n",
        "        img = (img / 255).astype(np.float32)\n",
        "\n",
        "        # Convert image from BGR to RGB\n",
        "        img = img[...,::-1]\n",
        "\n",
        "        # Make the image dataset squared\n",
        "        dim = min(img.shape[:-1])\n",
        "        img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2, (img.shape[1]-dim)//2:(img.shape[1]+dim)//2, :]\n",
        "\n",
        "        # Resize the image to 224x224 pixels\n",
        "        img = tfkl.Resizing(224, 224)(img)\n",
        "\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "# Load images from the 'animals/' folder\n",
        "animals_path = 'animals/'\n",
        "animals = load_images_from_folder(animals_path)\n",
        "\n",
        "# Load images from the 'items/' folder\n",
        "items_path = 'items/'\n",
        "items = load_images_from_folder(items_path)"
      ],
      "metadata": {
        "id": "kTgXBRWXne9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîé Inspect Data"
      ],
      "metadata": {
        "id": "oMivVZnSYwqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images to display\n",
        "num_img = 10\n",
        "\n",
        "# Create subplots for displaying items\n",
        "fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i%2, i%num_img//2]\n",
        "    ax.imshow(np.clip(items[i], 0, 255))  # Display clipped item images\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create subplots for displaying animals\n",
        "fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i%2, i%num_img//2]\n",
        "    ax.imshow(np.clip(animals[i], 0, 255))  # Display clipped animal images\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xejs_ca6OTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Process Data"
      ],
      "metadata": {
        "id": "cwkaQ15-Yzkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate 'animals' and 'items' arrays along axis 0\n",
        "X = np.concatenate([animals, items], axis=0)\n",
        "\n",
        "# Create labels: 1 for 'animals', 0 for 'items'\n",
        "y = np.concatenate([np.ones(len(animals)), np.zeros(len(items))], axis=0)\n",
        "\n",
        "y = tfk.utils.to_categorical(y,len(np.unique(y)))\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=30, stratify=np.argmax(y,axis=1))\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "EMshS7g3ne6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Define Network Parameters"
      ],
      "metadata": {
        "id": "hNUeDYCbJdWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape for the model\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Output shape for the model\n",
        "output_shape = y_train.shape[1]\n",
        "\n",
        "print(\"Input Shape:\", input_shape)\n",
        "print(\"Output Shape:\", output_shape)"
      ],
      "metadata": {
        "id": "RiU_CBZtJhWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Learning rate: step size for updating the model's weights\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Print the defined parameters\n",
        "print(\"Epochs:\", epochs)\n",
        "print(\"Batch Size:\", batch_size)\n",
        "print(\"Learning Rare:\", learning_rate)"
      ],
      "metadata": {
        "id": "RzgVYo9SJlV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Build the Model"
      ],
      "metadata": {
        "id": "Km2bBg3lnnsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape=input_shape,\n",
        "    output_shape=output_shape,\n",
        "    learning_rate=learning_rate,\n",
        "    augmentation=None,\n",
        "    seed=seed\n",
        "):\n",
        "    # Set the random seed for reproducibility\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Define the input layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    # Apply optional data augmentation, then first convolutional layer\n",
        "    if augmentation == None:\n",
        "        x = tfkl.Conv2D(filters=16, kernel_size=3, padding='same', name='conv0')(input_layer)\n",
        "    else:\n",
        "        x = augmentation(input_layer)\n",
        "        x = tfkl.Conv2D(filters=16, kernel_size=3, padding='same', name='conv0')(x)\n",
        "\n",
        "    # Apply activation and pooling after the first convolution\n",
        "    x = tfkl.ReLU(name='relu0')(x)\n",
        "    x = tfkl.MaxPooling2D(name='mp0')(x)\n",
        "\n",
        "    # Apply second convolutional layer, activation, and pooling\n",
        "    x = tfkl.Conv2D(filters=32, kernel_size=3, padding='same', name='conv1')(x)\n",
        "    x = tfkl.ReLU(name='relu1')(x)\n",
        "    x = tfkl.MaxPooling2D(name='mp1')(x)\n",
        "\n",
        "    # Apply third convolutional layer, activation, and pooling\n",
        "    x = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='conv2')(x)\n",
        "    x = tfkl.ReLU(name='relu2')(x)\n",
        "    x = tfkl.MaxPooling2D(name='mp2')(x)\n",
        "\n",
        "    # Apply fourth convolutional layer, activation, and pooling\n",
        "    x = tfkl.Conv2D(filters=128, kernel_size=3, padding='same', name='conv3')(x)\n",
        "    x = tfkl.ReLU(name='relu3')(x)\n",
        "    x = tfkl.Flatten(name='flatten')(x)\n",
        "\n",
        "    # Define the output layer with softmax activation for classification\n",
        "    output_layer = tfkl.Dense(units=2, activation='softmax', name='Output')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='CNN')\n",
        "\n",
        "    # Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "    loss = tfk.losses.CategoricalCrossentropy()\n",
        "    optimizer = tfk.optimizers.Adam(learning_rate)\n",
        "    metrics = ['accuracy']\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    # Return the compiled model\n",
        "    return model"
      ],
      "metadata": {
        "id": "4oX3N2r0ne4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with specified input and output shapes\n",
        "model = build_model()\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "GF-eEu0One18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Train the Model"
      ],
      "metadata": {
        "id": "zDn9YBTMKAbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the patience value for early stopping\n",
        "patience = 100\n",
        "\n",
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Store the callback in a list\n",
        "callbacks = [early_stopping]"
      ],
      "metadata": {
        "id": "2pxPV2UWRUep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with early stopping callback\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'CNN_'+str(final_val_accuracy)+'.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up resources\n",
        "del model"
      ],
      "metadata": {
        "id": "kigVjoDznezc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yvaj_UbDnew1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí™ Image Augmentaion"
      ],
      "metadata": {
        "id": "AarbrlVYoukT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL for the image\n",
        "url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStoEtY__xJfntgW3oCvg06vYkUZS24FTyoCQ&s'\n",
        "\n",
        "# Send a GET request to the URL and retrieve the image content\n",
        "response = requests.get(url)\n",
        "\n",
        "# Load the image and normalise pixel values\n",
        "img = np.array(Image.open(BytesIO(response.content))) / 255\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VK6sgg3zo3An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Flip"
      ],
      "metadata": {
        "id": "Vgv0bbylowLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with random flipping\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
        "])\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VpMFwSdgneui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Translation"
      ],
      "metadata": {
        "id": "GHoaZ7EApK-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with random translation\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomTranslation(0.2,0.2),\n",
        "])\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7xDawtRDner4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Rotation"
      ],
      "metadata": {
        "id": "2j2iSwhpref-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with random rotation\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wy8bm_2hrOAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Zoom"
      ],
      "metadata": {
        "id": "5k2MhuBHSMev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with random zoom\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aoPajXijSOR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Brightness"
      ],
      "metadata": {
        "id": "cm_K3YHjQ07a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with random brightness\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomBrightness(0.5, value_range=(0,1)),\n",
        "])\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cCbdMvQfQ1tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Contrast"
      ],
      "metadata": {
        "id": "pAf0hZmORMU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with random contrast\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomContrast(0.75),\n",
        "])\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nc5BtAaURMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All together"
      ],
      "metadata": {
        "id": "u5ea1XqLMJBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential model for image augmentation with various transformations\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tfkl.RandomTranslation(0.2,0.2),\n",
        "    tfkl.RandomRotation(0.2),\n",
        "    tfkl.RandomZoom(0.2),\n",
        "    tfkl.RandomBrightness(0.5, value_range=(0,1)),\n",
        "    tfkl.RandomContrast(0.75),\n",
        "], name='Augmentation')\n",
        "\n",
        "# Set up the figure and grid layout for displaying images\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
        "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
        "\n",
        "# Display the original image\n",
        "ax1 = fig.add_subplot(gs[0])\n",
        "ax1.imshow(np.squeeze(img))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Apply augmentation and display the first augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax2 = fig.add_subplot(gs[1])\n",
        "ax2.imshow(np.squeeze(augmented_img))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the second augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax3 = fig.add_subplot(gs[2])\n",
        "ax3.imshow(np.squeeze(augmented_img))\n",
        "ax3.axis('off')\n",
        "\n",
        "# Apply augmentation again and display the third augmented image\n",
        "augmented_img = np.clip(augmentation(img), 0., 1.)\n",
        "ax4 = fig.add_subplot(gs[3])\n",
        "ax4.imshow(np.squeeze(augmented_img))\n",
        "ax4.axis('off')\n",
        "\n",
        "# Show the figure with all images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F2qvwtGZMLME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Build the Model"
      ],
      "metadata": {
        "id": "mfNyFB3vrg8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a data augmentation pipeline with random flip, brightness, and translation\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal\"),\n",
        "    tfkl.RandomBrightness(0.2, value_range=(0,1)),\n",
        "    tfkl.RandomTranslation(0.2,0.2),\n",
        "], name='preprocessing')\n",
        "\n",
        "# Build the model with specified input and output shapes\n",
        "model = build_model(augmentation=augmentation)\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "ThUBdQkrMSJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Train the Model"
      ],
      "metadata": {
        "id": "8dKapEzpY9TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the patience value for early stopping\n",
        "patience = 100\n",
        "\n",
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Store the callback in a list\n",
        "callbacks = [early_stopping]"
      ],
      "metadata": {
        "id": "rrjQHUcVM8W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with early stopping callback\n",
        "aug_history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(aug_history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'AugCNN_'+str(final_val_accuracy)+'.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up resources\n",
        "del model"
      ],
      "metadata": {
        "id": "XSukGdqarDw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(aug_history['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n",
        "plt.plot(aug_history['val_loss'], label='CNN with Augmentation', alpha=.8, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(aug_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n",
        "plt.plot(aug_history['val_accuracy'], label='CNN with Augmentation', alpha=.8, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jD8u5utIrDuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üïπÔ∏è Use the Model - Make Inference"
      ],
      "metadata": {
        "id": "wddQZcTdFoQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tfk.models.load_model('AugCNN_93.33.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ],
      "metadata": {
        "id": "Ix4jFiU8FguL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities and get predicted classes\n",
        "test_predictions = model.predict(X_test, verbose=0)\n",
        "test_predictions = np.argmax(test_predictions, axis=-1)\n",
        "\n",
        "# Extract ground truth classes\n",
        "test_gt = np.argmax(y_test, axis=-1)\n",
        "\n",
        "# Calculate and display test set accuracy\n",
        "test_accuracy = accuracy_score(test_gt, test_predictions)\n",
        "print(f'Accuracy score over the test set: {round(test_accuracy, 4)}')\n",
        "\n",
        "# Calculate and display test set precision\n",
        "test_precision = precision_score(test_gt, test_predictions, average='weighted')\n",
        "print(f'Precision score over the test set: {round(test_precision, 4)}')\n",
        "\n",
        "# Calculate and display test set recall\n",
        "test_recall = recall_score(test_gt, test_predictions, average='weighted')\n",
        "print(f'Recall score over the test set: {round(test_recall, 4)}')\n",
        "\n",
        "# Calculate and display test set F1 score\n",
        "test_f1 = f1_score(test_gt, test_predictions, average='weighted')\n",
        "print(f'F1 score over the test set: {round(test_f1, 4)}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_gt, test_predictions)\n",
        "\n",
        "# Create labels combining confusion matrix values\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Plot the confusion matrix with class labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=labels, fmt='', xticklabels=['Item','Animal'], yticklabels=['Item','Animal'], cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XOQWmD4rcITs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü™Ñ Image Retrieval"
      ],
      "metadata": {
        "id": "C-4VyIrmFqXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding model by removing the last layer of the original model\n",
        "embedding = tfk.Sequential(model.layers[:-1])\n",
        "\n",
        "# Display the summary of the embedding model architecture\n",
        "embedding.summary()"
      ],
      "metadata": {
        "id": "Lp1AT_lULEca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and preprocess a single image for feature extraction\n",
        "index = 100\n",
        "image = np.expand_dims(X[index], axis=0)\n",
        "\n",
        "# Predict the features of the selected image using the embedding model\n",
        "image_features = embedding.predict(image, verbose=0)\n",
        "\n",
        "# Display the selected image\n",
        "plt.imshow(X[index])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "# Extract features from the entire dataset using the embedding model\n",
        "dataset_features = embedding.predict(X, batch_size=32, verbose=0)\n",
        "\n",
        "# Compute the distances between the selected image's features and the entire dataset's features\n",
        "distances = np.mean(np.square(dataset_features - image_features), axis=-1)\n",
        "\n",
        "# Sort images by their distances (similarity to the selected image)\n",
        "ordered_images = X[distances.argsort()]\n",
        "\n",
        "# Display the top 10 most similar images\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i % num_img]\n",
        "    ax.imshow(ordered_images[i])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gA-j4WLKLTB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use Pretrained Models as Image Search Engine"
      ],
      "metadata": {
        "id": "rqzJmIROcMfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MobileNetV2 model pre-trained on ImageNet, without the top classification layer\n",
        "mobilenet = tfk.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg',\n",
        ")\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "mobilenet.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(mobilenet, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "lA0wqw5KMB3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and preprocess a single image for feature extraction using MobileNetV2\n",
        "index = 100\n",
        "image = np.expand_dims(X[index], axis=0)\n",
        "\n",
        "# Preprocess the image and predict its features using the MobileNetV2 model\n",
        "image_features = mobilenet.predict(preprocess_input(image * 255), verbose=0)\n",
        "\n",
        "# Display the selected image\n",
        "plt.imshow(X[index])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "# Preprocess the dataset and extract features from all images using MobileNetV2\n",
        "dataset_features = mobilenet.predict(preprocess_input(X * 255), batch_size=32, verbose=0)\n",
        "\n",
        "# Compute the distances between the selected image's features and the entire dataset's features\n",
        "distances = np.mean(np.square(dataset_features - image_features), axis=-1)\n",
        "\n",
        "# Sort images by their distances (similarity to the selected image)\n",
        "ordered_images = X[distances.argsort()]\n",
        "\n",
        "# Display the top 10 most similar images\n",
        "num_img = 10\n",
        "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
        "for i in range(num_img):\n",
        "    ax = axes[i % num_img]\n",
        "    ax.imshow(ordered_images[i])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vEGVRvVGMCcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"15\"> **Instagram:** https://www.instagram.com/airlab_polimi/\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"15\"> **LinkedIn:** https://www.linkedin.com/company/airlab-polimi/\n",
        "___\n",
        "Credits: Eugenio Lomurno üìß eugenio.lomurno@polimi.it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "   Copyright 2024 Eugenio Lomurno\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```"
      ],
      "metadata": {
        "id": "L_akyV-3fDIn"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}