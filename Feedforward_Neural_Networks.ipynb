{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoeDebrois/Artificial-NN-and-Deep-Learning/blob/main/Feedforward_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Lecture 1: Feedforward Neural Networks"
      ],
      "metadata": {
        "id": "HfOcRE6NajAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üåê Connect Colab to Google Drive"
      ],
      "metadata": {
        "id": "BEf-L1xjdsl2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoaLQpvChLpb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2024-2025] AN2DL/Lecture 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3FoTyRa9pLu"
      },
      "source": [
        "### ‚öôÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "print(f\"TensorFlow version {tf.__version__}\")\n",
        "\n",
        "# Import other libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "QKhgM4fFCHrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è≥ Load Data"
      ],
      "metadata": {
        "id": "v9Wmh_xyaLXC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4TCo9ShRuxs"
      },
      "source": [
        "# Load the Iris dataset into a variable called 'data'\n",
        "data = load_iris()\n",
        "\n",
        "# Print the description of the Iris dataset\n",
        "print(data.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*f6KbPXwksAliMIsibFyGJw.png\" width=\"800\">"
      ],
      "metadata": {
        "id": "xGQ5FdGAcoil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé Inspect Data"
      ],
      "metadata": {
        "id": "43Pf5BCAEwtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame 'iris_dataset' from the Iris dataset\n",
        "iris_dataset = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "print('Iris dataset shape', iris_dataset.shape)\n",
        "\n",
        "# Display the first 10 rows of the Iris dataset\n",
        "iris_dataset.head(10)"
      ],
      "metadata": {
        "id": "0Y5brVXGDOlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJs0el5gSYa-"
      },
      "source": [
        "# Print the shape of the Iris dataset\n",
        "print('Iris dataset shape', iris_dataset.shape)\n",
        "\n",
        "# Generate summary statistics for the Iris dataset\n",
        "iris_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the target values from the Iris dataset\n",
        "target = data.target\n",
        "print('Target shape', target.shape)\n",
        "\n",
        "# Calculate the unique target labels and their counts\n",
        "unique, count = np.unique(target, return_counts=True)\n",
        "print('Target labels:', unique)\n",
        "for u in unique:\n",
        "    print(f'Class {unique[u]} has {count[u]} samples')"
      ],
      "metadata": {
        "id": "Y7q3rGR8DmgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the iris dataset\n",
        "plot_dataset = iris_dataset.copy()\n",
        "\n",
        "# Assign target labels to the dataset\n",
        "plot_dataset[\"Species\"] = target\n",
        "\n",
        "# Plot using seaborn pairplot\n",
        "sns.pairplot(plot_dataset, hue=\"Species\", palette=\"tab10\", markers=[\"o\", \"s\", \"D\"])\n",
        "plt.show()\n",
        "\n",
        "# Clean up by deleting the temporary dataset\n",
        "del plot_dataset"
      ],
      "metadata": {
        "id": "f5-5VcsinisH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split in train, validation and test"
      ],
      "metadata": {
        "id": "mRlqCCXas4TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into a combined training and validation set, and a separate test set\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    iris_dataset,\n",
        "    target,\n",
        "    test_size=20,\n",
        "    random_state=seed,\n",
        "    stratify=target\n",
        ")\n",
        "\n",
        "# Further split the combined training and validation set into a training set and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val,\n",
        "    y_train_val,\n",
        "    test_size=20,\n",
        "    random_state=seed,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print('Training set shape:\\t', X_train.shape, y_train.shape)\n",
        "print('Validation set shape:\\t', X_val.shape, y_val.shape)\n",
        "print('Test set shape:\\t\\t', X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "lH3D2rOAr1Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opLBOnWjWqay"
      },
      "source": [
        "### üîÑ Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLGTBSQ_s0zi"
      },
      "source": [
        "# Compute maximum values for each feature\n",
        "max_df = X_train.max()\n",
        "\n",
        "# Display maximum values\n",
        "print('Iris dataset maximum values')\n",
        "print(max_df)\n",
        "\n",
        "# Compute minimum values for each feature\n",
        "min_df = X_train.min()\n",
        "\n",
        "# Display minimum values\n",
        "print('\\nIris dataset minimum values')\n",
        "print(min_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalise training data\n",
        "X_train = (X_train - min_df) / (max_df - min_df)\n",
        "\n",
        "# Normalise validation data\n",
        "X_val = (X_val - min_df) / (max_df - min_df)\n",
        "\n",
        "# Normalise test data\n",
        "X_test = (X_test - min_df) / (max_df - min_df)"
      ],
      "metadata": {
        "id": "cYJHpJZmySFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics of the normalised training data\n",
        "X_train.describe()"
      ],
      "metadata": {
        "id": "OTGaohKvyTNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply one-hot encoding to training labels\n",
        "y_train = tfk.utils.to_categorical(y_train, num_classes=len(unique))\n",
        "\n",
        "# Apply one-hot encoding to validation labels\n",
        "y_val = tfk.utils.to_categorical(y_val, num_classes=len(unique))\n",
        "\n",
        "# Apply one-hot encoding to test labels\n",
        "y_test = tfk.utils.to_categorical(y_test, num_classes=len(unique))\n",
        "\n",
        "# Display shapes of the encoded label sets\n",
        "print('Training set target shape:\\t', y_train.shape)\n",
        "print('Validation set target shape:\\t', y_val.shape)\n",
        "print('Test set target shape:\\t\\t', y_test.shape)"
      ],
      "metadata": {
        "id": "PnVTcFNiIETT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the input shape, corresponding to the number of features\n",
        "input_shape = X_train.shape[1:]\n",
        "print(f'Input shape of the network {input_shape}')\n",
        "\n",
        "# Determine the output shape, corresponding to the number of classes\n",
        "output_shape = y_train.shape[1]\n",
        "print(f'Output shape of the network {output_shape}')"
      ],
      "metadata": {
        "id": "MT5mYq_byWXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßÆ Define Network Parameters"
      ],
      "metadata": {
        "id": "UwbhHmwlNwVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size: number of samples processed in each training iteration\n",
        "batch_size = 16\n",
        "\n",
        "# Number of epochs: times the entire dataset is passed through the network during training\n",
        "epochs = 500\n",
        "\n",
        "# Learning rate: step size for updating the model's weights\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "z3lLi4w9yXLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr9CX7CYdBg_"
      },
      "source": [
        "### üõ†Ô∏è Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkccDbv-bzKr"
      },
      "source": [
        "def build_model(\n",
        "    input_shape=input_shape,\n",
        "    output_shape=output_shape,\n",
        "    learning_rate=learning_rate,\n",
        "    seed=seed\n",
        "):\n",
        "\n",
        "    # Fix randomness\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Build the neural network layer by layer\n",
        "    inputs = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    # Add hidden layer with ReLU activation\n",
        "    x = tfkl.Dense(units=16, name='Hidden')(inputs)\n",
        "    x = tfkl.Activation('relu', name='HiddenActivation')(x)\n",
        "\n",
        "    # Add output layer with softmax activation\n",
        "    x = tfkl.Dense(units=output_shape, name='Output')(x)\n",
        "    outputs = tfkl.Activation('softmax', name='Softmax')(x)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs, name='FeedforwardNeuralNetwork')\n",
        "\n",
        "    # Compile the model with loss, optimizer, and metrics\n",
        "    loss = tfk.losses.CategoricalCrossentropy()\n",
        "    optimizer = tfk.optimizers.Adam(learning_rate)\n",
        "    metrics = ['accuracy']\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RRvC_IPbzH7"
      },
      "source": [
        "# Build the model with specified input and output shapes\n",
        "model = build_model()\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXmw4F0wlY0h"
      },
      "source": [
        "### üß† Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp4EZ3EvlbdE"
      },
      "source": [
        "# Train the model and store the training history\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val)\n",
        ").history\n",
        "\n",
        "# Calculate the final validation accuracy\n",
        "final_val_accuracy = round(history['val_accuracy'][-1] * 100, 2)\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = f'Iris_Feedforward_{final_val_accuracy}.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up memory resources\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSbqnzodn6tS"
      },
      "source": [
        "# Create a figure with two vertically stacked subplots\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(15, 6), sharex=True)\n",
        "\n",
        "# Plot training and validation loss\n",
        "ax1.plot(history['loss'], label='Training loss', alpha=.8)\n",
        "ax1.plot(history['val_loss'], label='Validation loss', alpha=.8)\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=.3)\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "ax2.plot(history['accuracy'], label='Training accuracy', alpha=.8)\n",
        "ax2.plot(history['val_accuracy'], label='Validation accuracy', alpha=.8)\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üïπÔ∏è Use the Model - Make Inference"
      ],
      "metadata": {
        "id": "zgpmJiGS1FrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Classification Metrics**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1dUVBtRp6yJmfr1-cwUmWtKX2UfgozNaj\" width=\"250\"/>\n",
        "\n",
        "$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "\n",
        "$\\text{Precision} = \\frac{TP}{TP + FP}$\n",
        "\n",
        "$\\text{Recall} = \\frac{TP}{TP + FN}$\n",
        "\n",
        "$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "---\n",
        "\n",
        "**Multiclass Classification Metrics**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yDfrbcjHPTqFx9P5ZPhi07S5XVcXjd6g\" width=\"300\"/>\n",
        "\n",
        "$\\text{Accuracy} = \\sum_{i=1}^{N} \\frac{TP_i}{TP_i + TN_i + FP_i + FN_i}$\n",
        "\n",
        "$\\text{Precision}_i = \\frac{TP_i}{TP_i + FP_i}$\n",
        "\n",
        "$\\text{Recall}_i = \\frac{TP_i}{TP_i + FN_i}$\n",
        "\n",
        "$F1_i = 2 \\cdot \\frac{\\text{Precision}_i \\cdot \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i}$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Macro Averaging**\n",
        "\n",
        "$\\text{Precision}_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Precision}_i$\n",
        "\n",
        "$\\text{Recall}_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Recall}_i$\n",
        "\n",
        "$F1_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} F1_i$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Weighted Macro Averaging**\n",
        "\n",
        "$\\text{Precision}_{weighted} = \\sum_{i=1}^{N} \\left( \\frac{N_i}{N} \\times \\text{Precision}_i \\right)$\n",
        "\n",
        "$\\text{Recall}_{weighted} = \\sum_{i=1}^{N} \\left( \\frac{N_i}{N} \\times \\text{Recall}_i \\right)$\n",
        "\n",
        "$F1_{weighted} = \\sum_{i=1}^{N} \\left( \\frac{N_i}{N} \\times F1_i \\right)$\n"
      ],
      "metadata": {
        "id": "oFgm8madBMvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tfk.models.load_model('Iris_Feedforward_95.0.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "ZeZyQ9frlIS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities and get predicted classes\n",
        "train_predictions = model.predict(X_train, verbose=0)\n",
        "train_predictions = np.argmax(train_predictions, axis=-1)\n",
        "\n",
        "# Extract ground truth classes\n",
        "train_gt = np.argmax(y_train, axis=-1)\n",
        "\n",
        "# Calculate and display training set accuracy\n",
        "train_accuracy = accuracy_score(train_gt, train_predictions)\n",
        "print(f'Accuracy score over the train set: {round(train_accuracy, 4)}')\n",
        "\n",
        "# Calculate and display training set precision\n",
        "train_precision = precision_score(train_gt, train_predictions, average='weighted')\n",
        "print(f'Precision score over the train set: {round(train_precision, 4)}')\n",
        "\n",
        "# Calculate and display training set recall\n",
        "train_recall = recall_score(train_gt, train_predictions, average='weighted')\n",
        "print(f'Recall score over the train set: {round(train_recall, 4)}')\n",
        "\n",
        "# Calculate and display training set F1 score\n",
        "train_f1 = f1_score(train_gt, train_predictions, average='weighted')\n",
        "print(f'F1 score over the train set: {round(train_f1, 4)}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(train_gt, train_predictions)\n",
        "\n",
        "# Create labels combining confusion matrix values\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Plot the confusion matrix with class labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=labels, fmt='', xticklabels=['Setosa', 'Versicolor', 'Virginica'], yticklabels=['Setosa', 'Versicolor', 'Virginica'], cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oghL4pJeuRfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities and get predicted classes\n",
        "val_predictions = model.predict(X_val, verbose=0)\n",
        "val_predictions = np.argmax(val_predictions, axis=-1)\n",
        "\n",
        "# Extract ground truth classes\n",
        "val_gt = np.argmax(y_val, axis=-1)\n",
        "\n",
        "# Calculate and display validation set accuracy\n",
        "val_accuracy = accuracy_score(val_gt, val_predictions)\n",
        "print(f'Accuracy score over the validation set: {round(val_accuracy, 4)}')\n",
        "\n",
        "# Calculate and display validation set precision\n",
        "val_precision = precision_score(val_gt, val_predictions, average='weighted')\n",
        "print(f'Precision score over the validation set: {round(val_precision, 4)}')\n",
        "\n",
        "# Calculate and display validation set recall\n",
        "val_recall = recall_score(val_gt, val_predictions, average='weighted')\n",
        "print(f'Recall score over the validation set: {round(val_recall, 4)}')\n",
        "\n",
        "# Calculate and display validation set F1 score\n",
        "val_f1 = f1_score(val_gt, val_predictions, average='weighted')\n",
        "print(f'F1 score over the validation set: {round(val_f1, 4)}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(val_gt, val_predictions)\n",
        "\n",
        "# Create labels combining confusion matrix values\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Plot the confusion matrix with class labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=labels, fmt='', xticklabels=['Setosa', 'Versicolor', 'Virginica'], yticklabels=['Setosa', 'Versicolor', 'Virginica'], cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UCotSTSp1QBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities and get predicted classes\n",
        "test_predictions = model.predict(X_test, verbose=0)\n",
        "test_predictions = np.argmax(test_predictions, axis=-1)\n",
        "\n",
        "# Extract ground truth classes\n",
        "test_gt = np.argmax(y_test, axis=-1)\n",
        "\n",
        "# Calculate and display test set accuracy\n",
        "test_accuracy = accuracy_score(test_gt, test_predictions)\n",
        "print(f'Accuracy score over the test set: {round(test_accuracy, 4)}')\n",
        "\n",
        "# Calculate and display test set precision\n",
        "test_precision = precision_score(test_gt, test_predictions, average='weighted')\n",
        "print(f'Precision score over the test set: {round(test_precision, 4)}')\n",
        "\n",
        "# Calculate and display test set recall\n",
        "test_recall = recall_score(test_gt, test_predictions, average='weighted')\n",
        "print(f'Recall score over the test set: {round(test_recall, 4)}')\n",
        "\n",
        "# Calculate and display test set F1 score\n",
        "test_f1 = f1_score(test_gt, test_predictions, average='weighted')\n",
        "print(f'F1 score over the test set: {round(test_f1, 4)}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_gt, test_predictions)\n",
        "\n",
        "# Create labels combining confusion matrix values\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Plot the confusion matrix with class labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=labels, fmt='', xticklabels=['Setosa', 'Versicolor', 'Virginica'], yticklabels=['Setosa', 'Versicolor', 'Virginica'], cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "urxJdHb-wHAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìú TODO Exercise\n",
        "Perform the same classification analysis on the Penguins dataset to predict the correct species\n",
        "\n",
        "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"800\">\n",
        "\n",
        "```\n",
        "...\n",
        "\n",
        "# Load Data\n",
        "os.environ[\"DATASET_NAME\"] = \"penguins.csv\"\n",
        "os.environ[\"DATASET_URL\"] = \"1qn1P6_KW08wGRfSkTlzBoDCyVb18T3Lk\"\n",
        "if not os.path.exists(os.environ[\"DATASET_NAME\"]):\n",
        "    print(\"Downloading data...\")\n",
        "    ! gdown -q ${DATASET_URL}\n",
        "    print(\"Download completed\")\n",
        "else:\n",
        "    print(\"Data already downloaded. Using cached data...\")\n",
        "dataset = pd.read_csv('penguins.csv')\n",
        "\n",
        "# Inspect Data\n",
        "...\n",
        "\n",
        "# Process Data\n",
        "...\n",
        "\n",
        "\n",
        "# Build the Model\n",
        "...\n",
        "\n",
        "# Train the Model\n",
        "...\n",
        "\n",
        "# Use the Model - Make Inferece\n",
        "...\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4LWsw6r9PuJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"15\"> **Instagram:** https://www.instagram.com/airlab_polimi/\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"15\"> **LinkedIn:** https://www.linkedin.com/company/airlab-polimi/\n",
        "___\n",
        "Credits: Eugenio Lomurno üìß eugenio.lomurno@polimi.it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "   Copyright 2024 Eugenio Lomurno\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```\n"
      ],
      "metadata": {
        "id": "aov5Pu91TldT"
      }
    }
  ]
}