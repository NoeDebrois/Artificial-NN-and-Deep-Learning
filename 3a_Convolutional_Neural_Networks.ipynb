{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoeDebrois/Artificial-NN-and-Deep-Learning/blob/main/3a_Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojlbdyI38RJR"
      },
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Lecture 3a: Convolutional Neural Networks\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ffjpWwLSS5XwEW2GVW0D_hpwgSw1Mf3g\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üåê Connect Colab to Google Drive"
      ],
      "metadata": {
        "id": "_GckmKyTuHdT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoaLQpvChLpb",
        "outputId": "15abc3ed-bf09-4ae0-9f53-854e488450cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/Colab Notebooks/Artificial NN & Deep Learning/Exercise Session 3\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/Colab Notebooks/Artificial NN & Deep Learning/Exercise Session 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Import Libraries"
      ],
      "metadata": {
        "id": "fNC-mMrJNSXU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_S1JfaW8bIN"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcLWIZfBUd80"
      },
      "source": [
        "## ‚è≥ Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr7OCTrKUYnm"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset and split into training-validation and test sets\n",
        "(X_train_val, y_train_val), (X_test, y_test) = tfk.datasets.cifar10.load_data()\n",
        "\n",
        "# Define a mapping of labels to their corresponding digit names\n",
        "labels = {0:'Airplane', 1:'Automobile', 2:'Bird', 3:'Cat', 4:'Deer', 5:'Dog', 6:'Frog', 7:'Horse', 8:'Ship', 9:'Truck'}\n",
        "\n",
        "# Save unique labels\n",
        "unique_labels = list(labels.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZSXl5tywEFn"
      },
      "source": [
        "## üîé Inspect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgwE4S6YVGCH"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the loaded datasets\n",
        "print(\"Training-Validation Data Shape:\", X_train_val.shape)\n",
        "print(\"Training-Validation Label Shape:\", y_train_val.shape)\n",
        "print(\"Test Data Shape:\", X_test.shape)\n",
        "print(\"Test Label Shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0FqHc5kUfo4"
      },
      "outputs": [],
      "source": [
        "# Display a sample of images from the training-validation dataset\n",
        "num_img = 10\n",
        "random_indices = random.sample(range(len(X_train_val)), num_img)\n",
        "\n",
        "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
        "\n",
        "# Iterate through the selected number of images\n",
        "for i, idx in enumerate(random_indices):\n",
        "    ax = axes[i % num_img]\n",
        "    ax.imshow(np.squeeze(X_train_val[idx]), vmin=0., vmax=1.)\n",
        "    ax.set_title(f'{labels[y_train_val[idx][0]]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust layout and display the images\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCqd-PjruouG"
      },
      "outputs": [],
      "source": [
        "# Inspect the target\n",
        "print('Counting occurrences of target classes:')\n",
        "print(pd.DataFrame(y_train_val, columns=['digit'])['digit'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtSzHoNCVIQb"
      },
      "source": [
        "## üîÑ Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjMOZHmHxRdL"
      },
      "outputs": [],
      "source": [
        "# Normalize data to the range [0, 1]\n",
        "X_train_val = (X_train_val / 255).astype('float32')\n",
        "X_test = (X_test / 255).astype('float32')\n",
        "\n",
        "# Convert labels to categorical format using one-hot encoding\n",
        "y_train_val = tfk.utils.to_categorical(y_train_val)\n",
        "y_test = tfk.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRo4lvg-xRaP"
      },
      "outputs": [],
      "source": [
        "# Split data into training and validation sets, maintaining class distribution\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=0.1, stratify=y_train_val)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"Training Data Shape:\", X_train.shape)\n",
        "print(\"Training Label Shape:\", y_train.shape)\n",
        "print(\"Validation Data Shape:\", X_val.shape)\n",
        "print(\"Validation Label Shape:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "700y5071uorK"
      },
      "outputs": [],
      "source": [
        "# Compare categorical label and \"default\" label representation\n",
        "# Display the categorical label\n",
        "print('Categorical label:', y_train[0])\n",
        "# Display the equivalent numeric label\n",
        "print('\"Default\" label:', np.argmax(y_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s70orp99V8G0"
      },
      "source": [
        "## üëÅÔ∏è Convolutional Neural Networks\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/format:webp/1*oB3S5yHHhvougJkPXuc8og.gif\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5MeXe-vCVIl"
      },
      "source": [
        "#### **What is a \"Convolution\"?**\n",
        "\n",
        "A convolution is a mathematical operation on two objects, an input information and a kernel, to produce an outcome that expresses how the shape of one is modified by the other\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*LkBTMMY49OTF-I84yESMjQ.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc6Y60ZiJ4Ic"
      },
      "source": [
        "**Kernel (or Filter)**\n",
        "\n",
        "Since the RGB and greyscale channels of an image are in 2D dimensions, the kernel of a convolution that operates in the image domain has the form of a 2D matrix\n",
        "\n",
        "<img src=\"https://courses.cs.washington.edu/courses/cse446/21au/sections/08/correlation.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MyPChBNHKfj"
      },
      "source": [
        "The output of a convolution operation is called a feature map, while the convolution kernel is often referred to as the filter, channel or feature detector of a convolution\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:750/format:webp/0*0Mukvf3OWXtmFJp3\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoiSDj6gCW-P"
      },
      "source": [
        "**Convolutional Layer**\n",
        "\n",
        "A \"Convolutional Layer\" is a set of trainable kernels that act as a feature extractor, given a set of input data, a problem to be solved, and an optimisation strategy\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AS6aFoW8sq7kt5vAiP_5bA.png\" width=\"500\">\n",
        "\n",
        "The number of kernels, called filters in TensorFlow, is not the only descriptive parameter of a convolutional layer. Among the most important are the strides and the padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HuGf79vo_Up"
      },
      "source": [
        "**Strides**\n",
        "\n",
        "The strides are the number of cells that the filter moves at each iteration. The usefulness of having strides greater than one is to prevent the output feature map from being too similar to the input, while at the same time reducing its size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5FZGzWmCW7A"
      },
      "source": [
        "**Padding**\n",
        "\n",
        "Padding is the size of the frame - typically consisting of zeros - to surround the input. The utility of such a frame is to change the size of the output feature map to control its size. It is typically used in conjunction with steps to one to make the input and feature map the same size\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tq_lyA2uRy4BTBpYlbKTTQ.gif\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpQ6T1YLCW3p"
      },
      "source": [
        "Whereas for dense or fully-connected neural networks the number of parameters between two layers is given by the product of the number of neurons in the first layer and the number of neurons in the second layer plus one for bias, the parameters of a convolutional layer are a function of filter size, strides and padding\n",
        "\n",
        "Given $C_{in}$ as the number of input channels, $f_h$ and $f_w$ as the filter dimensions, height and width respectively, and $C_{out}$ as the number of output channels, the number of parameters of a convolutional layer can be calculated as follows\n",
        "\n",
        "**Parameters =** $(f_h \\times f_w \\times C_{in} + 1) \\times C_{out}$\n",
        "\n",
        "Finally, with a padding value of $p$ and a step size equal to $s$, the output shape of a convolutional layer can be computed as\n",
        "\n",
        "**Output Shape =** $(\\frac{C_{in} + 2p - f_h}{s}+1,\\frac{C_{in} + 2p - f_w}{s}+1,C_{out})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlQHifcsXWqc"
      },
      "source": [
        "**Pooling Layer**\n",
        "\n",
        "The pooling operation built into the pooling layer is an aparametric transformation whose purpose is to reduce the spatial dimensionality of the input tensor - in other words, to reduce the height and width of the feature maps - typically through maximum or average operations. Again, there are stride and kernel size parameters, but by convention the value 2 is used in both cases\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-3-9b0tAakAsdozzhNlEww.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P38StW6cbbZS"
      },
      "source": [
        "**Flatten Layer**\n",
        "\n",
        "The purpose of the flattening operation is to 'unwind' the filters computed by the convolution and pooling layers so that they can be processed by one or more fully connected layers, thus enabling prediction in the context of classification and regression.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IWUxuBpqn2VuV-7Ubr01ng.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR3tC0g3p4Cm"
      },
      "source": [
        "## üßÆ Define Network Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQtRpb13ZYYU"
      },
      "outputs": [],
      "source": [
        "# Input shape for the model\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Output shape for the model\n",
        "output_shape = y_train.shape[1]\n",
        "\n",
        "print(\"Input Shape:\", input_shape)\n",
        "print(\"Output Shape:\", output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnCi66geMzjf"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 128\n",
        "\n",
        "# Learning rate: step size for updating the model's weights\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Print the defined parameters\n",
        "print(\"Epochs:\", epochs)\n",
        "print(\"Batch Size:\", batch_size)\n",
        "print(\"Learning Rare:\", learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trMoDWu0VUWr"
      },
      "source": [
        "## üõ†Ô∏è Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGXALNI5cufT"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/publication/345788674/figure/fig1/AS:957248478117889@1605237230738/Overview-and-details-of-a-convolutional-neural-network-CNN-architecture-for-image_W640.jpg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGvtyzwHM2_7"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape=input_shape,\n",
        "    output_shape=output_shape,\n",
        "    learning_rate=learning_rate,\n",
        "    seed=seed\n",
        "):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Build the neural network layer by layer\n",
        "    inputs = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    x = tfkl.Conv2D(filters=32, kernel_size=3, padding='same', name='conv1')(inputs)\n",
        "    x = tfkl.Activation('relu', name='act1')(x)\n",
        "    x = tfkl.MaxPooling2D(pool_size=2, name='mp1')(x)\n",
        "\n",
        "    x = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='conv2')(x)\n",
        "    x = tfkl.Activation('relu', name='act2')(x)\n",
        "    x = tfkl.MaxPooling2D(pool_size=2, name='mp2')(x)\n",
        "\n",
        "    x = tfkl.Flatten(name='flatten')(x)\n",
        "\n",
        "    x = tfkl.Dense(units=output_shape, name='dense')(x)\n",
        "    outputs = tfkl.Activation('softmax', name='softmax')(x)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs, name='CNN')\n",
        "\n",
        "    # Compile the model\n",
        "    loss = tfk.losses.CategoricalCrossentropy()\n",
        "    optimizer = tfk.optimizers.Adam(learning_rate)\n",
        "    metrics = ['accuracy']\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8E8YVf8M6bA"
      },
      "outputs": [],
      "source": [
        "# Build the model with specified input and output shapes\n",
        "model = build_model()\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hKARwDKelr1"
      },
      "source": [
        "## üß† Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the patience value for early stopping\n",
        "patience = 20\n",
        "\n",
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Store the callback in a list\n",
        "callbacks = [early_stopping]"
      ],
      "metadata": {
        "id": "QYBPO7AFAtDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACPB2NrUNY3R"
      },
      "outputs": [],
      "source": [
        "# Train the model with early stopping callback\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'CIFAR10_CNN_'+str(final_val_accuracy)+'.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up resources\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aohG90lKsGE5"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(15, 2))\n",
        "plt.plot(history['loss'], label='Training loss', alpha=.8)\n",
        "plt.plot(history['val_loss'], label='Validation loss', alpha=.8)\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(15, 2))\n",
        "plt.plot(history['accuracy'], label='Training accuracy', alpha=.8)\n",
        "plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.8)\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InOZkhX-roVc"
      },
      "source": [
        "## üïπÔ∏è Use the Model - Make Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4jBhFkoYhRo"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "model = tfk.models.load_model('CIFAR10_CNN_69.92.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWV-o4zQlY2"
      },
      "outputs": [],
      "source": [
        "def extract_activations(model, X, num_images):\n",
        "\n",
        "    # Identify the first convolutional layer\n",
        "    first_conv_index = None\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, tfk.layers.Conv2D):\n",
        "            first_conv_index = i\n",
        "            break\n",
        "\n",
        "    if first_conv_index is None:\n",
        "        raise ValueError(\"The model does not contain a 2D convolution\")\n",
        "\n",
        "    # Extract activations from the first convolutional layer\n",
        "    first_conv = tfk.Sequential(model.layers[:first_conv_index + 1])\n",
        "    first_activations = first_conv(X[:num_images])\n",
        "\n",
        "    # Identify the first pooling layer after the first convolution\n",
        "    pooling_index = None\n",
        "    for i, layer in enumerate(model.layers[first_conv_index + 1:], start=first_conv_index + 1):\n",
        "        if isinstance(layer, (tfk.layers.MaxPooling2D, tfk.layers.AveragePooling2D)):\n",
        "            pooling_index = i\n",
        "            break\n",
        "\n",
        "    if pooling_index is None:\n",
        "        raise ValueError(\"The model does not contain a 2D pooling operation after the first convolution\")\n",
        "\n",
        "    # Extract activations from the first convolution and the first pooling layer\n",
        "    second_conv = tfk.Sequential(model.layers[:pooling_index + 1])\n",
        "    second_activations = second_conv(X[:num_images])\n",
        "\n",
        "    return first_activations, second_activations\n",
        "\n",
        "def find_last_conv_layer(model):\n",
        "\n",
        "    # Identify the last convolutional layer in the model\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, tfk.layers.Conv2D):\n",
        "            return layer.name\n",
        "    raise ValueError(\"No Conv2D layer found in the model.\")\n",
        "\n",
        "def visualize(model, X, y, unique_labels, num_images=50, display_activations=True):\n",
        "\n",
        "    # Extract activations from the model\n",
        "    first_activations, second_activations = extract_activations(model, X, num_images)\n",
        "\n",
        "    # Select a random image for prediction and visualisation\n",
        "    image = np.random.randint(0, num_images)\n",
        "    predictions = model.predict(np.expand_dims(X[image], axis=0), verbose=0)\n",
        "    class_int = np.argmax(predictions[0])\n",
        "    class_str = unique_labels[class_int]\n",
        "\n",
        "    # Create figure layout for displaying the image and predictions\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(16, 4))\n",
        "    gs = gridspec.GridSpec(1, 2, figure=fig, width_ratios=[1.5, 1.5], wspace=0)\n",
        "\n",
        "    # Display the selected image with the true class\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    ax1.set_title(f\"True class: {unique_labels[np.argmax(y[image])]}\", loc='left')\n",
        "    if X[image].shape[-1] == 1:\n",
        "        ax1.imshow(np.squeeze(X[image]), cmap='bone', vmin=0., vmax=1.)\n",
        "    else:\n",
        "        ax1.imshow(np.squeeze(X[image]), vmin=0., vmax=1.)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Display the prediction bar\n",
        "    ax2 = fig.add_subplot(gs[1])\n",
        "    ax2.barh(unique_labels, np.squeeze(predictions, axis=0), color=plt.get_cmap('tab10').colors)\n",
        "    ax2.set_title(f\"Predicted class: {class_str} (Confidence: {max(np.squeeze(predictions, axis=0)):.2f})\", loc='left')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.set_xlim(0.0, 1.0)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Display activations if required\n",
        "    if display_activations:\n",
        "        # Visualise the activations from the first convolutional layer\n",
        "        fig, axes = plt.subplots(1, 8, figsize=(16, 14))\n",
        "        for i in range(8):\n",
        "            ax = axes[i]\n",
        "            ax.imshow(first_activations[image, :, :, i], cmap='bone', vmin=0., vmax=1.)\n",
        "            ax.axis('off')\n",
        "            if i == 0:\n",
        "                ax.set_title('First convolution activations', loc='left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Visualise the activations from the first pooling layer\n",
        "        fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n",
        "        for i in range(16):\n",
        "            ax = axes[i // 8, i % 8]\n",
        "            ax.imshow(second_activations[image, :, :, i], cmap='bone', vmin=0., vmax=1.)\n",
        "            ax.axis('off')\n",
        "            if i == 0:\n",
        "                ax.set_title('Second convolution activations', loc='left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "visualize(model, X_test, y_test, unique_labels, display_activations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8RzaYIasRWx"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the entire test set\n",
        "predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5qpA6JPsRUi"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to class labels\n",
        "pred_classes = np.argmax(predictions, axis=-1)\n",
        "\n",
        "# Extract ground truth classes\n",
        "true_classes = np.argmax(y_test, axis=-1)\n",
        "\n",
        "# Calculate and display test set accuracy\n",
        "accuracy = accuracy_score(true_classes, pred_classes)\n",
        "print(f'Accuracy score over the test set: {round(accuracy, 4)}')\n",
        "\n",
        "# Calculate and display test set precision\n",
        "precision = precision_score(true_classes, pred_classes, average='weighted')\n",
        "print(f'Precision score over the test set: {round(precision, 4)}')\n",
        "\n",
        "# Calculate and display test set recall\n",
        "recall = recall_score(true_classes, pred_classes, average='weighted')\n",
        "print(f'Recall score over the test set: {round(recall, 4)}')\n",
        "\n",
        "# Calculate and display test set F1 score\n",
        "f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
        "print(f'F1 score over the test set: {round(f1, 4)}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(true_classes, pred_classes)\n",
        "\n",
        "# Combine numbers and percentages into a single string for annotation\n",
        "annot = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, annot=annot, fmt='', xticklabels=list(labels.values()), yticklabels=list(labels.values()), cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd0yRTHMiml0"
      },
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"15\"> **Instagram:** https://www.instagram.com/airlab_polimi/\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"15\"> **LinkedIn:** https://www.linkedin.com/company/airlab-polimi/\n",
        "___\n",
        "Credits: Eugenio Lomurno üìß eugenio.lomurno@polimi.it\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "   Copyright 2024 Eugenio Lomurno\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}